{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd34ea97405b3902d4925a75b46b982e",
     "grade": false,
     "grade_id": "cell-0a8316b039d048ad",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 1. A multilayer perceptron network\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "The goal of this exercise is to get familiar with the basics of PyTorch and train a simple feedforward network on a real-world data set. If you are not familiar with PyTorch, there is a number of good tutorials [here](https://pytorch.org/tutorials/index.html). We recommend the following ones:\n",
    "* [What is PyTorch?](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)\n",
    "* [Autograd: Automatic Differentiation](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)\n",
    "* [Learning PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n",
    "* [Neural Networks](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py)\n",
    "\n",
    "This exercise consists of several tasks which require some background knowledge of basic ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e2970339980ef7d85c3754662c4ee8",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "351dfcae9c688c72cab8ba805d82882c",
     "grade": false,
     "grade_id": "cell-01498dc3f73989b5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# Select data directory\n",
    "import os\n",
    "if os.path.isdir('/coursedata'):\n",
    "    course_data_dir = '/coursedata'\n",
    "elif os.path.isdir('../data'):\n",
    "    course_data_dir = '../data'\n",
    "else:\n",
    "    # Specify course_data_dir on your machine\n",
    "    # course_data_dir = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9948c1cbf3480ee5b2a9c2a397921ec",
     "grade": false,
     "grade_id": "cell-7643a1c4336569b4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device which you are going to use for training\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc5c0195206dae40876fe429916217c4",
     "grade": false,
     "grade_id": "cell-70232a39ccf9c751",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c86a077d5d534679bd3f597ac6a6bdce",
     "grade": false,
     "grade_id": "cell-ce13efdf413792bd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "We are going to use *winequality* dataset which contains red and white vinho verde wine samples rated by experts from 0 to 10 (obtained from [here](https://archive.ics.uci.edu/ml/datasets/wine+quality)). We will transform the task into a binary classification problem and try to predict if the quality of wine is greater or lower than 7. The idea is to compare the quality of predictions obtained by a random forest classfier and a simple neural network.\n",
    "\n",
    "Let us load the data and split it into the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e33090c95b149fec5aec93c5e199a4f",
     "grade": false,
     "grade_id": "cell-bc3a08be26d3616e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from /coursedata/winequality\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(course_data_dir, 'winequality')\n",
    "print('Data loaded from %s' % data_dir)\n",
    "\n",
    "df = pd.concat([\n",
    "    pd.read_csv(os.path.join(data_dir, 'winequality-red.csv'), delimiter=';'),\n",
    "    pd.read_csv(os.path.join(data_dir, 'winequality-white.csv'), delimiter=';')\n",
    "])\n",
    "\n",
    "x = df.loc[:, df.columns != 'quality'].values\n",
    "y = df['quality'].values >= 7  # Convert to a binary classification problem\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1, shuffle=True)\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "356da8829de19d89765b10c3ff4aabb3",
     "grade": false,
     "grade_id": "cell-fff001b57c687c28",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Train a Random forest classifier\n",
    "\n",
    "*In the code below, train a random forest classifier from sklearn (look at the description [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)) using `x_train` and `y_train` with 100 trees. Name your classifier object `classifier`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97dc62b85ec7d20b7554e453e3e33fd5",
     "grade": false,
     "grade_id": "cell-92782487f52a6c3e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06627359  0.09031844  0.07708119  0.08215981  0.08724307  0.0771777\n",
      "  0.07972623  0.1145798   0.08106524  0.07956871  0.16480621]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "# classifier = ...\n",
    "# YOUR CODE HERE\n",
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "print(classifier.feature_importances_)\n",
    "print(classifier.predict([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc8909cddf05a5f36de2499f483edede",
     "grade": true,
     "grade_id": "rf_accuracy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random forest: 0.89\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94      1064\n",
      "           1       0.78      0.58      0.67       236\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      1300\n",
      "   macro avg       0.85      0.77      0.80      1300\n",
      "weighted avg       0.89      0.89      0.89      1300\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(15,0.5,'true labels')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARAAAAEWCAYAAACuU8gIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAFdNJREFUeJzt3Xl4FeXZx/HvDUEJO4gbiAgI+qpFVNS6Vqu4L9jWVwVcqdQFLXUBF0QBtWp5fd3qAuIGCuJW96ogsqOgKGBFVAjIquwJASTh7h9nQg80JCcPnExO8vtc11yZmeeZOfcB8uOZ5Zwxd0dEJES1uAsQkcylABGRYAoQEQmmABGRYAoQEQmmABGRYAqQKsLMss3sbTNbbWavbMd+OpvZhzuytriY2XFm9m3cdWQy030gFYuZdQJuAPYHcoEvgXvcffx27vdi4DrgaHcv2O5CKzgzc6C1u38fdy2VmUYgFYiZ3QA8BNwL7A7sDTwOnLsDdt8cmF0VwiMVZpYVdw2VgrtrqgATUB/IA84voc/OJAJmUTQ9BOwctZ0ALABuBH4CFgOXR219gV+AjdFrdAXuAoYm7XsfwIGsaPkyYA6JUdBcoHPS+vFJ2x0NTAFWRz+PTmr7BOgPTIj28yHQeBvvraj+nkn1dwTOAGYDK4DbkvofAUwCVkV9HwN2itrGRu9lbfR+L0jafy9gCTCkaF20TavoNQ6NlpsAy4AT4v63UZGn2AvQFP1FwGlAQdEv8Db69AMmA7sBuwITgf5R2wnR9v2AGtEvXj7QMGrfOjC2GSBAbWANsF/UtidwYDS/OUCARsBK4OJou4ui5V2i9k+AH4A2QHa0fN823ltR/X2i+q8EfgZeAuoCBwLrgZZR/8OAX0evuw/wDdAjaX8O7FvM/u8nEcTZyQES9bky2k8t4ANgQNz/Lir6pEOYimMXYJmXfIjRGejn7j+5+88kRhYXJ7VvjNo3uvt7JP733S+wnk3AQWaW7e6L3f3rYvqcCXzn7kPcvcDdhwGzgLOT+jzr7rPdfR0wAmhXwmtuJHG+ZyMwHGgMPOzuudHrfw20BXD3z919cvS6OcBTwG9SeE93uvuGqJ4tuPsg4DvgUxKheXsp+6vyFCAVx3KgcSnH5k2AeUnL86J1m/exVQDlA3XKWoi7ryUx7L8KWGxm75rZ/inUU1RT06TlJWWoZ7m7F0bzRb/gS5Pa1xVtb2ZtzOwdM1tiZmtInDdqXMK+AX529/Wl9BkEHAQ86u4bSulb5SlAKo5JJIboHUvos4jEydAie0frQqwlMVQvskdyo7t/4O4dSPxPPIvEL1Zp9RTVtDCwprJ4gkRdrd29HnAbYKVsU+IlRzOrQ+K80mDgLjNrtCMKrcwUIBWEu68mcfz/dzPraGa1zKyGmZ1uZg9E3YYBvc1sVzNrHPUfGviSXwLHm9neZlYfuLWowcx2N7NzzKw2sIHEoVBhMft4D2hjZp3MLMvMLgAOAN4JrKks6pI4T5MXjY6u3qp9KdCyjPt8GPjc3f8IvAs8ud1VVnIKkArE3R8kcQ9IbxInEH8EugP/iLrcDUwFpgMzgC+idSGv9RHwcrSvz9nyl74aias5i0hcmfgNcE0x+1gOnBX1XU7iCspZ7r4spKYyugnoROLqziAS7yXZXcDzZrbKzP63tJ2Z2bkkTmRfFa26ATjUzDrvsIorId1IJiLBNAIRkWAKEBEJpgARkWAKEBEJVmE/UJR9SHed3c1QK6c8FncJsh1qZpV6P81mGoGISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEU4CISDAFiIgEy4q7gMriyTs7c/rxB/Hzilzan38vAA3r1WLI/VfQvEkj5i1aQZeeg1mVu44LT2/PDZd1AGDtug1cf+/LzJi9EID6dbJ54s5OHNBqT9zhqr4v8un0ubG9r6puw4YNXH5JZzb+8gsFhYV0OOVUrul+PZ9OnsSDAx7AN20iu1Yt+t9zH3s3bx53ueXO3D3uGoqVfUj3ilnYNhxzaCvW5m/g6f6XbA6Qe/58LivX5DPg2Y+46fIONKhbi96PvMmvD27BrDlLWJW7jlOOOYDefzqD4y8ZAMCgfhczYdr3PPfGJGpkVadWzZ1YnbcuzrdWZiunPBZ3CTuMu7MuP59atWuzceNGLru4E71uvZ3bb+3Fw48+TstWrXh52IvMnDGD/vfeF3e5O0TNLCzVvmk7hDGz/c2sl5k9YmYPR/P/k67Xi9uEL35gxer8LdaddUJbhr79KQBD3/6Us09sC8Dkr+ayKjcRCp9Nn0vT3RsAULd2TY49tBXPvTEJgI0FhRkXHpWNmVGrdm0ACgoKKCgoADPMIG9tHgB5eXnsuttucZYZm7QcwphZL+AiYDjwWbR6L2CYmQ1398oR1aXYbZe6LFm2BoAly9awa6O6/9Xnso5H88GEfwHQoukuLFuZx8C+XfhVm6ZM++ZHbnrgVfLX/1KudcuWCgsLuej83zF//nwuuKgTbdsezF397qH7Vd3YuebO1KldhyHDRsRdZizSNQLpChzu7ve5+9Boug84Imorlpl1M7OpZja1YNnXaSqt4ji+fWsu7XgUvR9+E4CsrOq0278Zg14Zx1EX3U/+ug3cdEWHmKuU6tWrM+L1N/nw4zHMnDGd776bzZAXnuOxJwfy0cdjOfe83zHggb/GXWYs0hUgm4AmxazfM2orlrsPdPf27t4+q/GBaSqt/Py0PJc9GtcDYI/G9fh5Re7mtoNaN+GJPp04/y8DWbF6LQALl65k4U+rmDJzHgBvjPySdvs3K//CpVj16tXj8COOZMK4scz+dhZt2x4MwKmnncFX06bFXF080hUgPYBRZva+mQ2Mpn8Co4A/p+k1K5x3x8ygy9lHAtDl7CN555PpADTboyHDB1xJ1zte4Pv5P23uv3R5LguWrKR188Tx9AlH7MesOUvKv3DZbMWKFaxZkzgMXb9+PZMnTaRFy1bk5eaSk5O4OjZp0gRatGwVZ5mxSdtVGDOrRuKQpSlgwAJgirsXprJ9pl2Fef6vl3HcYa1p3KAOP61YQ/8n3+Pt0dMZev8VNNuzIT8uXknnnoNZuSafx/t0ouNJ7Zi/eAUABYWbOLbzAwC0bdOUx+/szE5Z1clZuIxudw7dfMI1U1SmqzCzv51F79tuYdOmQjZtck459TSuuqY7o0Z+xOOPPUI1M+rVr0/f/veyV7PKMVosy1UYXcaVHa4yBUhVVCEu44pI5acAEZFgChARCaYAEZFgChARCaYAEZFgChARCaYAEZFgChARCaYAEZFgChARCaYAEZFgChARCaYAEZFgChARCaYAEZFgChARCaYAEZFgChARCaYAEZFgChARCaYAEZFgChARCaYAEZFgpQaImR1jZrWj+S5m9qCZNU9/aSJS0aUyAnkCyDezg4GewDzghbRWJSIZIZUAKfDE8y/PBR5294eBuuktS0QyQVYKfXLN7FagC3C8mVUHaqS3LBHJBKmMQC4ANgBd3X0J0BT4W1qrEpGMUOoIJAqNB5OW56NzICJCCQFiZrmAF9cEuLvXS1tVIpIRthkg7q4TpSJSopRuJDOzY83s8mi+sZm1SG9ZIpIJUrmR7E6gF3BrtGonYGg6ixKRzJDKCOQ84BxgLYC7L0L3gYgIqQXIL9GNZA5QdFu7iEgqATLCzJ4CGpjZlcBIYFB6yxKRTJDKfSADzKwDsAZoA/Rx94/SXpmIVHip3MoOMAPIJnEYMyN95YhIJknlKswfgc+A3wF/ACab2RXpLkxEKr5URiA3A4e4+3IAM9sFmAg8k87CRKTiS+Uk6gIgN2k5F/gxPeWISCYp6bMwN0SzC4FPzexNEudAziVxSCMiVVxJhzBFN4v9EE1F3kxfOSKSSUr6MF3f8ixERDJPqSdRzWxXEt+FeiBQs2i9u/82jXWJSAZI5STqi8AsoAXQF8gBpqSxJhHJEKkEyC7uPhjY6O5j3P0K4NdprktEMkAq94FsjH4uNrMzgUXAXukrSUQyhSU+aFtCB7OzgHFAM+BRoB7Q193fSmdhy/IKSi5MKqz1GzfFXYJsh70a7mSp9i01QOKiAMlcCpDMVpYAKelGskcp/kuVAXD368tYl4hUMiWdA5lablWISEYq6Uay58uzEBHJPCl9K7uISHEUICISTAEiIsFS+UayNmY2ysxmRsttzax3+ksTkYoulRHIIBIPldoI4O7TgQvTWZSIZIZUAqSWu2/9BUIF6ShGRDJLKgGyzMxa8Z8HS/0BWJzWqkQkI6TyYbprgYHA/ma2EJgLdElrVSKSEVJ5sNQc4OTokZbV3D23tG1EpGpI5RvJ+my1DIC790tTTSKSIVI5hFmbNF8TOAv4Jj3liEgmKfPH+c1sZ+Atdz81PSUl6OP8mUsf589sZfk4f8idqLWAlgHbiUglk8o5kBn853tBqgO7Ajr/ISIpnQM5K2m+AFjq7rqRTERKDhAzqwa86+4HlVM9IpJBSjwH4u6bgK/MbO9yqkdEMkgqhzB7Al+b2WckXdJ193PSVpWIZIRUAkTPyBWRYqUSIGe4e6/kFWZ2PzAmPSWJSKZI5T6QDsWsO31HFyIimaek58JcDVwDtDSz6UlNdYEJ6S5MRCq+bd7Kbmb1gYbAX4Fbkppy3X1FugvTreyZS7eyZzY92lJipQDJbOn+LIyICKAAEZHtoAARkWAKEBEJpgARkWAKEBEJpgARkWAKEBEJpgARkWAKEBEJpgARkWAKEBEJpgARkWAKEBEJpgARkWAKEBEJpgARkWAKEBEJpgARkWAKEBEJpgARkWCpPJlOttOIl4bw1j9exd0557w/cEGnS/hu9iz+dm8/1uXns2eTJtx59wPUrlMn7lIF+NvddzB5wlgaNGzE4JfeAODZpx5lwtjRVKtWjQYNG9HzjrtpvOtuvDz0WUZ98C4AhYWFzM+Zw2vvj6Ve/fpxvoVyo8c6pNmc77+jz2038fTzw8mqUYMbr/sTN93ah7tuv5nuPW7mkMMO5503X2fRwgV0u+b6uMvdITL9sQ7Tp02lZnYt7u93++YAWbs2j9q1EwH/+ssvMi/nB/7Sq88W200c9wmvDR/C//19cLnXvCPpsQ4VSM7cORx40MHUzM4mKyuLdoe2Z+zokcyfl0O7Q9sDcPiRRzHm449irlSKtD2kPfXqbTmCKAoPgPXr12H89+/Y6I/e47cdqtZTXxUgadZy3335atpUVq9axfp165g0YRxLly6hZavWjB8zGoDRIz9g6dIlMVcqpRn8xCNceM7JjPrgXS7rdu0WbevXr2PK5Akcd2Jxj5KuvMo9QMzs8hLaupnZVDOb+sIzg8qzrLTZp0UrOl/alR7X/JEbrvsT+7bZj+rVq3Nbn/68NmIYV3Q+n/z8fGrUqBF3qVKKrldfz/C3RnLSqWfyj1eHbdE2adwYDvzVIVXm3EeROE6i9gWeLa7B3QcCA6HynAMBOLvj7zm74+8BePKxh9htt91p3qIlDz2eCMn583KYOH5MnCVKGZx0yhncduO1XHblf0Yho0e+z29PqVqHL5CmEYiZTd/GNAPYPR2vWZGtXLEcgCWLFzHm45GcfNoZm9dt2rSJ5wc/RcffXxBniVKKBfPnbZ6fOG40zZq32Lycl5fL9GlTOfr4E+MoLVbpGoHsDpwKrNxqvQET0/SaFdZtN/dgzepVZGVlceMtvalXrz4jXhrC668khsG/OfFkzjznvJirlCJ339GTr76YwupVq7jg7JO49Mpr+WziOH6cn4OZsfseTejR647N/cd/MorDjjia7OxaMVYdj7RcxjWzwcCz7j6+mLaX3L1TafuoTIcwVU2mX8at6spyGVf3gcgOpwDJbLoPRETKhQJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmAJERIIpQEQkmLl73DVUSWbWzd0Hxl2HhNHfX4JGIPHpFncBsl3094cCRES2gwJERIIpQOJT5Y+fM5z+/tBJVBHZDhqBiEgwBYiIBFOAxMDMTjOzb83sezO7Je56JHVm9oyZ/WRmM+OupSJQgJQzM6sO/B04HTgAuMjMDoi3KimD54DT4i6iolCAlL8jgO/dfY67/wIMB86NuSZJkbuPBVbEXUdFoQApf02BH5OWF0TrRDKOAqT8WTHrdC1dMpICpPwtAJolLe8FLIqpFpHtogApf1OA1mbWwsx2Ai4E3oq5JpEgCpBy5u4FQHfgA+AbYIS7fx1vVZIqMxsGTAL2M7MFZtY17pripFvZRSSYRiAiEkwBIiLBFCAiEkwBIiLBFCAiEkwBIsHMLC/62cTMXi2lbw8zq5W0/J6ZNUh3jZJeuowrWzCz6u5emGLfPHevk2LfHKC9uy/bnvqkYtEIpAoxs33MbJaZPW9m083sVTOrZWY5ZtbHzMYD55tZKzP7p5l9bmbjzGz/aPsWZjbJzKaYWf+t9jszmq9uZgPMbEb0GteZ2fVAE2C0mY2O+uWYWeNo/gYzmxlNPZL2+Y2ZDTKzr83sQzPLjtquN7N/RfsfXq5/iLIld9dURSZgHxIf3DsmWn4GuAnIAXom9RsFtI7mjwQ+jubfAi6J5q8F8pL2OzOavxp4DciKlhtFP3OAxkmvkQM0Bg4DZgC1gTrA18Ah0T4LgHZR/xFAl2h+EbBzNN8g7j/XqjxpBFL1/OjuE6L5ocCx0fzLAGZWBzgaeMXMvgSeAvaM+hwDDIvmh2xj/ycDT3riln3cvbTvzjgWeMPd17p7HvA6cFzUNtfdv4zmPycRKgDTgRfNrAuJkJGYZMVdgJS7rU96FS2vjX5WA1a5e7sUt9+apdBn6/7bsiFpvhDIjubPBI4HzgHuMLMDiwJLypdGIFXP3mZ2VDR/ETA+udHd1wBzzex8AEs4OGqeQOLTwwCdt7H/D4GrzCwr2r5RtD4XqFtM/7FAx+hcTG3gPGDctoo3s2pAM3cfDfQEGpA49JEYKECqnm+AS81sOtAIeKKYPp2Brmb2FYlzEkVfufhn4FozmwLU38b+nwbmA9Oj7TtF6wcC7xedRC3i7l+Q+J7Rz4BPgafdfVoJ9VcHhprZDGAa8P/uvqqE/pJGuoxbhZjZPsA77n5QzKVIJaERiIgE0whERIJpBCIiwRQgIhJMASIiwRQgIhJMASIiwf4NVshd+unhYbkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the accuracy of the random forest classifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "pred_test = classifier.predict(x_test)  # Predict labels of test data using the trained classifier\n",
    "c_matrix = confusion_matrix(y_test, pred_test) \n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, pred_test)\n",
    "print(\"Accuracy of random forest: {:.2f}\".format(rf_accuracy))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, pred_test))\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(4, 4))\n",
    "ax.set_title(\"Confusion matrix\")\n",
    "sns.heatmap(c_matrix, cmap='Blues', annot=True, fmt='g', cbar=False)\n",
    "ax.set_xlabel('predictions')\n",
    "ax.set_ylabel('true labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ecc61bbd856ff9141c9d0de211523be2",
     "grade": false,
     "grade_id": "cell-4d94c36293bfa64e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As you can see, the random forest classifier works quite nicely in this task without much tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd8228d2186021aafe8fd809c2f229fa",
     "grade": false,
     "grade_id": "cell-76070c68689a5242",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## A multilayer perceptron (MLP) network with two hidden layers\n",
    "\n",
    "In the code below, define a neural network architecture with:\n",
    "- input dimensionality 11\n",
    "- one hidden layer with 100 units with ReLU nonlinearity\n",
    "- one hidden layer with 100 units with ReLU nonlinearity\n",
    "- linear output layer with output dimensionality 2.\n",
    "\n",
    "**Please do not use [`torch.nn.Sequential`](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential) in your code.**\n",
    "\n",
    "You may want to look at [this tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py) for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4838e01b283d2060c905d954da14d45d",
     "grade": false,
     "grade_id": "cell-c648be60aebb3433",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "n_inputs = 11\n",
    "hidden_layer_units = 100\n",
    "output_dimensionality = 2\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.lin1 = nn.Linear(n_inputs, hidden_layer_units)\n",
    "        self.lin2 = nn.Linear(hidden_layer_units, hidden_layer_units)\n",
    "        self.lin3 = nn.Linear(hidden_layer_units, output_dimensionality)\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "        \n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ec8a74e5889cec8c4cbc26bf8f79a54",
     "grade": true,
     "grade_id": "mlp_architecture",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let us create the network and make sure it can process a random input of the right shape\n",
    "mlp = MLP()\n",
    "y = mlp(torch.randn(10, n_inputs))\n",
    "assert y.shape == torch.Size([10, 2]), \"Bad shape of y: y.shape={}\".format(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (lin1): Linear(in_features=11, out_features=100, bias=True)\n",
       "  (lin2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (lin3): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c0975e13adc5dc011d2a5c49ce44813",
     "grade": false,
     "grade_id": "cell-4a3bd5bb745e59c6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "One can also create an instance of a simple deep network using [`torch.nn.Sequential`](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential).\n",
    "\n",
    "In the cell below, please use [`torch.nn.Sequential`](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential)\n",
    "to create an MLP with the same structure as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "009519638addd8d21556ddc8039f8523",
     "grade": false,
     "grade_id": "cell-1310c88e86ee652b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# This function should return an MLP model created with torch.nn.Sequential\n",
    "# - input dimensionality 11\n",
    "# - one hidden layer with 100 units with ReLU nonlinearity\n",
    "# - one hidden layer with 100 units with ReLU nonlinearity\n",
    "# - linear output layer with output dimensionality 2.\n",
    "# mlp_seq = nn.Sequential(...\n",
    "# YOUR CODE HERE\n",
    "mlp_seq = nn.Sequential(nn.Linear(n_inputs, hidden_layer_units), nn.ReLU(),\n",
    "                        nn.Linear(hidden_layer_units, hidden_layer_units), nn.ReLU(),\n",
    "                        nn.Linear(hidden_layer_units, output_dimensionality))\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ef31af1e5f75043643a15dde8696fe7",
     "grade": false,
     "grade_id": "cell-9aff5dc7e6aa2c3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=11, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print the network\n",
    "print(mlp_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a6a6b214f3be7581a24ed938d6a2883",
     "grade": true,
     "grade_id": "mlp_Sequential",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let us feed a random input of the right shape to the network created with torch.nn.Sequential.\n",
    "y = mlp_seq(torch.randn(10, n_inputs))\n",
    "assert y.shape == torch.Size([10, 2]), \"Bad shape of y: y.shape={}\".format(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0150363a62c9680543f7697e313a5675",
     "grade": false,
     "grade_id": "cell-70cbd420870116d7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Train an MLP network\n",
    "\n",
    "Next we will train the multilayer perceptron network. For better understanding of the training process you can take a look at [this part of the tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#backprop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5fa7a839ecd25901b63e914bd53ec6c",
     "grade": false,
     "grade_id": "cell-0c16db048e765d97",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Data scaling\n",
    "\n",
    "Even though deep learning is supposed to work well on raw data without much feature engineering, it is usually a good idea to pre-process data so that the inputs have zero mean and unit standard deviation. PyTorch has its own tools for preprocessing but let us use sklearn's `StandardScaler` for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ca402794b8fd94bda252ab568e6a620",
     "grade": false,
     "grade_id": "cell-db601500e1d7cd93",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2af318ff16206e2fe8425c37fac0204c",
     "grade": false,
     "grade_id": "cell-5a5f4579c7df733f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Training loop\n",
    "\n",
    "Let us implement the training loop. We will use the Adam optimizer with learning rate 0.01 and we will process the data in the full-batch model (without splitting the data into mini-batches).\n",
    "\n",
    "*Your task is to insert the missing code. You may find it useful to look at [this tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py).*\n",
    "Your should have the following steps:\n",
    "* Transform `x_train_scaled` and `y_train` to `torch.tensor`, make sure the tensors have proper types and they go to the specified `device`.\n",
    "* Set all gradient values to zeros.\n",
    "* Calculate outputs of the MLP network (call them `outputs`).\n",
    "* Calculate cross entropy loss using [`torch.nn.functional.cross_entropy`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.cross_entropy).\n",
    "* Backpropagate the loss: compute the gradients of the loss wrt to all the parameters of the MLP.\n",
    "* Update the parameters of the model using the `optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0c2fc142dd0cdfa9399db8ec15f7f3d",
     "grade": false,
     "grade_id": "cell-692ef1b990bd1bbc",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0: Loss: 0.740367 Train accuracy 0.27 Test accuracy 0.82\n",
      "Train Epoch 1: Loss: 0.586770 Train accuracy 0.80 Test accuracy 0.82\n",
      "Train Epoch 2: Loss: 0.510217 Train accuracy 0.80 Test accuracy 0.82\n",
      "Train Epoch 3: Loss: 0.486345 Train accuracy 0.80 Test accuracy 0.82\n",
      "Train Epoch 4: Loss: 0.487904 Train accuracy 0.80 Test accuracy 0.82\n",
      "Train Epoch 5: Loss: 0.485306 Train accuracy 0.80 Test accuracy 0.82\n",
      "Train Epoch 6: Loss: 0.471999 Train accuracy 0.80 Test accuracy 0.82\n",
      "Train Epoch 7: Loss: 0.454140 Train accuracy 0.80 Test accuracy 0.82\n",
      "Train Epoch 8: Loss: 0.438104 Train accuracy 0.80 Test accuracy 0.82\n",
      "Train Epoch 9: Loss: 0.426995 Train accuracy 0.80 Test accuracy 0.82\n",
      "Train Epoch 10: Loss: 0.420718 Train accuracy 0.80 Test accuracy 0.82\n",
      "Train Epoch 11: Loss: 0.417463 Train accuracy 0.80 Test accuracy 0.82\n",
      "Train Epoch 12: Loss: 0.415271 Train accuracy 0.80 Test accuracy 0.83\n",
      "Train Epoch 13: Loss: 0.412850 Train accuracy 0.81 Test accuracy 0.83\n",
      "Train Epoch 14: Loss: 0.409618 Train accuracy 0.81 Test accuracy 0.83\n",
      "Train Epoch 15: Loss: 0.405737 Train accuracy 0.81 Test accuracy 0.83\n",
      "Train Epoch 16: Loss: 0.401665 Train accuracy 0.82 Test accuracy 0.83\n",
      "Train Epoch 17: Loss: 0.398007 Train accuracy 0.82 Test accuracy 0.83\n",
      "Train Epoch 18: Loss: 0.395177 Train accuracy 0.82 Test accuracy 0.83\n",
      "Train Epoch 19: Loss: 0.393243 Train accuracy 0.82 Test accuracy 0.83\n",
      "Train Epoch 20: Loss: 0.391856 Train accuracy 0.82 Test accuracy 0.83\n",
      "Train Epoch 21: Loss: 0.390484 Train accuracy 0.82 Test accuracy 0.84\n",
      "Train Epoch 22: Loss: 0.388788 Train accuracy 0.82 Test accuracy 0.83\n",
      "Train Epoch 23: Loss: 0.386796 Train accuracy 0.83 Test accuracy 0.84\n",
      "Train Epoch 24: Loss: 0.384813 Train accuracy 0.82 Test accuracy 0.84\n",
      "Train Epoch 25: Loss: 0.383207 Train accuracy 0.83 Test accuracy 0.83\n",
      "Train Epoch 26: Loss: 0.382023 Train accuracy 0.83 Test accuracy 0.83\n",
      "Train Epoch 27: Loss: 0.381107 Train accuracy 0.83 Test accuracy 0.83\n",
      "Train Epoch 28: Loss: 0.380270 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 29: Loss: 0.379345 Train accuracy 0.82 Test accuracy 0.82\n",
      "Train Epoch 30: Loss: 0.378209 Train accuracy 0.82 Test accuracy 0.82\n",
      "Train Epoch 31: Loss: 0.376905 Train accuracy 0.82 Test accuracy 0.82\n",
      "Train Epoch 32: Loss: 0.375550 Train accuracy 0.82 Test accuracy 0.82\n",
      "Train Epoch 33: Loss: 0.374270 Train accuracy 0.82 Test accuracy 0.82\n",
      "Train Epoch 34: Loss: 0.373109 Train accuracy 0.82 Test accuracy 0.82\n",
      "Train Epoch 35: Loss: 0.372054 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 36: Loss: 0.371012 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 37: Loss: 0.369912 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 38: Loss: 0.368787 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 39: Loss: 0.367677 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 40: Loss: 0.366631 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 41: Loss: 0.365656 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 42: Loss: 0.364728 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 43: Loss: 0.363802 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 44: Loss: 0.362821 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 45: Loss: 0.361804 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 46: Loss: 0.360773 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 47: Loss: 0.359764 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 48: Loss: 0.358791 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 49: Loss: 0.357824 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 50: Loss: 0.356820 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 51: Loss: 0.355793 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 52: Loss: 0.354776 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 53: Loss: 0.353801 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 54: Loss: 0.352868 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 55: Loss: 0.351900 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 56: Loss: 0.350893 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 57: Loss: 0.349853 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 58: Loss: 0.348787 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 59: Loss: 0.347713 Train accuracy 0.83 Test accuracy 0.83\n",
      "Train Epoch 60: Loss: 0.346646 Train accuracy 0.83 Test accuracy 0.82\n",
      "Train Epoch 61: Loss: 0.345582 Train accuracy 0.83 Test accuracy 0.83\n",
      "Train Epoch 62: Loss: 0.344511 Train accuracy 0.83 Test accuracy 0.83\n",
      "Train Epoch 63: Loss: 0.343457 Train accuracy 0.84 Test accuracy 0.83\n",
      "Train Epoch 64: Loss: 0.342404 Train accuracy 0.84 Test accuracy 0.83\n",
      "Train Epoch 65: Loss: 0.341361 Train accuracy 0.84 Test accuracy 0.83\n",
      "Train Epoch 66: Loss: 0.340311 Train accuracy 0.84 Test accuracy 0.83\n",
      "Train Epoch 67: Loss: 0.339248 Train accuracy 0.84 Test accuracy 0.83\n",
      "Train Epoch 68: Loss: 0.338163 Train accuracy 0.84 Test accuracy 0.83\n",
      "Train Epoch 69: Loss: 0.337067 Train accuracy 0.84 Test accuracy 0.83\n",
      "Train Epoch 70: Loss: 0.336014 Train accuracy 0.84 Test accuracy 0.84\n",
      "Train Epoch 71: Loss: 0.334968 Train accuracy 0.84 Test accuracy 0.84\n",
      "Train Epoch 72: Loss: 0.333897 Train accuracy 0.84 Test accuracy 0.84\n",
      "Train Epoch 73: Loss: 0.332856 Train accuracy 0.84 Test accuracy 0.84\n",
      "Train Epoch 74: Loss: 0.331810 Train accuracy 0.84 Test accuracy 0.84\n",
      "Train Epoch 75: Loss: 0.330733 Train accuracy 0.84 Test accuracy 0.84\n",
      "Train Epoch 76: Loss: 0.329671 Train accuracy 0.84 Test accuracy 0.84\n",
      "Train Epoch 77: Loss: 0.328636 Train accuracy 0.84 Test accuracy 0.83\n",
      "Train Epoch 78: Loss: 0.327619 Train accuracy 0.84 Test accuracy 0.83\n",
      "Train Epoch 79: Loss: 0.326598 Train accuracy 0.84 Test accuracy 0.83\n",
      "Train Epoch 80: Loss: 0.325552 Train accuracy 0.84 Test accuracy 0.83\n",
      "Train Epoch 81: Loss: 0.324502 Train accuracy 0.85 Test accuracy 0.83\n",
      "Train Epoch 82: Loss: 0.323452 Train accuracy 0.85 Test accuracy 0.83\n",
      "Train Epoch 83: Loss: 0.322421 Train accuracy 0.85 Test accuracy 0.83\n",
      "Train Epoch 84: Loss: 0.321376 Train accuracy 0.85 Test accuracy 0.83\n",
      "Train Epoch 85: Loss: 0.320316 Train accuracy 0.85 Test accuracy 0.84\n",
      "Train Epoch 86: Loss: 0.319258 Train accuracy 0.85 Test accuracy 0.83\n",
      "Train Epoch 87: Loss: 0.318162 Train accuracy 0.85 Test accuracy 0.83\n",
      "Train Epoch 88: Loss: 0.317066 Train accuracy 0.85 Test accuracy 0.84\n",
      "Train Epoch 89: Loss: 0.315969 Train accuracy 0.85 Test accuracy 0.84\n",
      "Train Epoch 90: Loss: 0.314873 Train accuracy 0.85 Test accuracy 0.84\n",
      "Train Epoch 91: Loss: 0.313756 Train accuracy 0.85 Test accuracy 0.84\n",
      "Train Epoch 92: Loss: 0.312596 Train accuracy 0.86 Test accuracy 0.84\n",
      "Train Epoch 93: Loss: 0.311406 Train accuracy 0.86 Test accuracy 0.84\n",
      "Train Epoch 94: Loss: 0.310210 Train accuracy 0.85 Test accuracy 0.84\n",
      "Train Epoch 95: Loss: 0.308984 Train accuracy 0.86 Test accuracy 0.84\n",
      "Train Epoch 96: Loss: 0.307727 Train accuracy 0.85 Test accuracy 0.84\n",
      "Train Epoch 97: Loss: 0.306490 Train accuracy 0.86 Test accuracy 0.84\n",
      "Train Epoch 98: Loss: 0.305234 Train accuracy 0.85 Test accuracy 0.84\n",
      "Train Epoch 99: Loss: 0.304006 Train accuracy 0.85 Test accuracy 0.84\n",
      "Train Epoch 100: Loss: 0.302739 Train accuracy 0.86 Test accuracy 0.84\n",
      "Train Epoch 101: Loss: 0.301508 Train accuracy 0.86 Test accuracy 0.84\n",
      "Train Epoch 102: Loss: 0.300245 Train accuracy 0.86 Test accuracy 0.84\n",
      "Train Epoch 103: Loss: 0.298966 Train accuracy 0.86 Test accuracy 0.84\n",
      "Train Epoch 104: Loss: 0.297700 Train accuracy 0.86 Test accuracy 0.84\n",
      "Train Epoch 105: Loss: 0.296405 Train accuracy 0.86 Test accuracy 0.84\n",
      "Train Epoch 106: Loss: 0.295080 Train accuracy 0.86 Test accuracy 0.84\n",
      "Train Epoch 107: Loss: 0.293773 Train accuracy 0.86 Test accuracy 0.84\n",
      "Train Epoch 108: Loss: 0.292426 Train accuracy 0.86 Test accuracy 0.84\n",
      "Train Epoch 109: Loss: 0.291091 Train accuracy 0.87 Test accuracy 0.84\n",
      "Train Epoch 110: Loss: 0.289727 Train accuracy 0.87 Test accuracy 0.84\n",
      "Train Epoch 111: Loss: 0.288365 Train accuracy 0.87 Test accuracy 0.84\n",
      "Train Epoch 112: Loss: 0.287138 Train accuracy 0.87 Test accuracy 0.85\n",
      "Train Epoch 113: Loss: 0.286076 Train accuracy 0.87 Test accuracy 0.84\n",
      "Train Epoch 114: Loss: 0.285109 Train accuracy 0.87 Test accuracy 0.85\n",
      "Train Epoch 115: Loss: 0.283231 Train accuracy 0.87 Test accuracy 0.84\n",
      "Train Epoch 116: Loss: 0.281412 Train accuracy 0.87 Test accuracy 0.84\n",
      "Train Epoch 117: Loss: 0.280585 Train accuracy 0.87 Test accuracy 0.84\n",
      "Train Epoch 118: Loss: 0.279649 Train accuracy 0.87 Test accuracy 0.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 119: Loss: 0.277915 Train accuracy 0.87 Test accuracy 0.84\n",
      "Train Epoch 120: Loss: 0.276309 Train accuracy 0.87 Test accuracy 0.84\n",
      "Train Epoch 121: Loss: 0.275649 Train accuracy 0.87 Test accuracy 0.84\n",
      "Train Epoch 122: Loss: 0.274431 Train accuracy 0.88 Test accuracy 0.84\n",
      "Train Epoch 123: Loss: 0.272592 Train accuracy 0.88 Test accuracy 0.85\n",
      "Train Epoch 124: Loss: 0.271432 Train accuracy 0.88 Test accuracy 0.84\n",
      "Train Epoch 125: Loss: 0.270308 Train accuracy 0.88 Test accuracy 0.84\n",
      "Train Epoch 126: Loss: 0.269275 Train accuracy 0.88 Test accuracy 0.85\n",
      "Train Epoch 127: Loss: 0.268076 Train accuracy 0.88 Test accuracy 0.84\n",
      "Train Epoch 128: Loss: 0.266376 Train accuracy 0.88 Test accuracy 0.84\n",
      "Train Epoch 129: Loss: 0.265014 Train accuracy 0.88 Test accuracy 0.85\n",
      "Train Epoch 130: Loss: 0.263973 Train accuracy 0.88 Test accuracy 0.85\n",
      "Train Epoch 131: Loss: 0.262844 Train accuracy 0.88 Test accuracy 0.85\n",
      "Train Epoch 132: Loss: 0.261935 Train accuracy 0.88 Test accuracy 0.85\n",
      "Train Epoch 133: Loss: 0.259948 Train accuracy 0.88 Test accuracy 0.84\n",
      "Train Epoch 134: Loss: 0.258456 Train accuracy 0.88 Test accuracy 0.85\n",
      "Train Epoch 135: Loss: 0.257603 Train accuracy 0.89 Test accuracy 0.85\n",
      "Train Epoch 136: Loss: 0.256305 Train accuracy 0.89 Test accuracy 0.85\n",
      "Train Epoch 137: Loss: 0.254874 Train accuracy 0.89 Test accuracy 0.85\n",
      "Train Epoch 138: Loss: 0.253791 Train accuracy 0.89 Test accuracy 0.85\n",
      "Train Epoch 139: Loss: 0.252782 Train accuracy 0.89 Test accuracy 0.85\n",
      "Train Epoch 140: Loss: 0.251870 Train accuracy 0.89 Test accuracy 0.85\n",
      "Train Epoch 141: Loss: 0.251066 Train accuracy 0.89 Test accuracy 0.85\n",
      "Train Epoch 142: Loss: 0.249615 Train accuracy 0.89 Test accuracy 0.84\n",
      "Train Epoch 143: Loss: 0.248619 Train accuracy 0.89 Test accuracy 0.85\n",
      "Train Epoch 144: Loss: 0.246937 Train accuracy 0.89 Test accuracy 0.85\n",
      "Train Epoch 145: Loss: 0.244881 Train accuracy 0.89 Test accuracy 0.85\n",
      "Train Epoch 146: Loss: 0.243715 Train accuracy 0.89 Test accuracy 0.85\n",
      "Train Epoch 147: Loss: 0.242961 Train accuracy 0.89 Test accuracy 0.85\n",
      "Train Epoch 148: Loss: 0.242123 Train accuracy 0.89 Test accuracy 0.85\n",
      "Train Epoch 149: Loss: 0.241177 Train accuracy 0.89 Test accuracy 0.85\n",
      "Train Epoch 150: Loss: 0.239988 Train accuracy 0.89 Test accuracy 0.85\n",
      "Train Epoch 151: Loss: 0.239165 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 152: Loss: 0.237571 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 153: Loss: 0.235545 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 154: Loss: 0.234492 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 155: Loss: 0.233722 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 156: Loss: 0.232330 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 157: Loss: 0.231187 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 158: Loss: 0.230662 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 159: Loss: 0.230362 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 160: Loss: 0.230354 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 161: Loss: 0.230463 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 162: Loss: 0.227933 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 163: Loss: 0.225694 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 164: Loss: 0.224035 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 165: Loss: 0.223370 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 166: Loss: 0.223696 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 167: Loss: 0.221843 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 168: Loss: 0.219412 Train accuracy 0.91 Test accuracy 0.86\n",
      "Train Epoch 169: Loss: 0.218639 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 170: Loss: 0.218167 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 171: Loss: 0.217836 Train accuracy 0.91 Test accuracy 0.86\n",
      "Train Epoch 172: Loss: 0.216872 Train accuracy 0.90 Test accuracy 0.85\n",
      "Train Epoch 173: Loss: 0.215245 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 174: Loss: 0.213516 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 175: Loss: 0.212506 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 176: Loss: 0.211525 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 177: Loss: 0.210994 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 178: Loss: 0.210732 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 179: Loss: 0.210041 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 180: Loss: 0.209188 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 181: Loss: 0.207953 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 182: Loss: 0.206426 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 183: Loss: 0.204665 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 184: Loss: 0.203548 Train accuracy 0.92 Test accuracy 0.85\n",
      "Train Epoch 185: Loss: 0.202944 Train accuracy 0.92 Test accuracy 0.85\n",
      "Train Epoch 186: Loss: 0.202590 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 187: Loss: 0.202589 Train accuracy 0.92 Test accuracy 0.85\n",
      "Train Epoch 188: Loss: 0.202755 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 189: Loss: 0.203069 Train accuracy 0.92 Test accuracy 0.85\n",
      "Train Epoch 190: Loss: 0.201649 Train accuracy 0.91 Test accuracy 0.85\n",
      "Train Epoch 191: Loss: 0.198984 Train accuracy 0.92 Test accuracy 0.85\n",
      "Train Epoch 192: Loss: 0.197072 Train accuracy 0.92 Test accuracy 0.85\n",
      "Train Epoch 193: Loss: 0.197138 Train accuracy 0.91 Test accuracy 0.86\n",
      "Train Epoch 194: Loss: 0.196918 Train accuracy 0.92 Test accuracy 0.85\n",
      "Train Epoch 195: Loss: 0.195760 Train accuracy 0.92 Test accuracy 0.85\n",
      "Train Epoch 196: Loss: 0.194633 Train accuracy 0.92 Test accuracy 0.86\n",
      "Train Epoch 197: Loss: 0.193471 Train accuracy 0.92 Test accuracy 0.85\n",
      "Train Epoch 198: Loss: 0.193145 Train accuracy 0.92 Test accuracy 0.86\n",
      "Train Epoch 199: Loss: 0.192548 Train accuracy 0.92 Test accuracy 0.86\n",
      "Train Epoch 200: Loss: 0.190196 Train accuracy 0.92 Test accuracy 0.86\n",
      "Train Epoch 201: Loss: 0.188935 Train accuracy 0.92 Test accuracy 0.85\n",
      "Train Epoch 202: Loss: 0.189764 Train accuracy 0.92 Test accuracy 0.86\n",
      "Train Epoch 203: Loss: 0.188479 Train accuracy 0.92 Test accuracy 0.86\n",
      "Train Epoch 204: Loss: 0.186158 Train accuracy 0.92 Test accuracy 0.86\n",
      "Train Epoch 205: Loss: 0.186066 Train accuracy 0.93 Test accuracy 0.85\n",
      "Train Epoch 206: Loss: 0.185620 Train accuracy 0.92 Test accuracy 0.86\n",
      "Train Epoch 207: Loss: 0.183951 Train accuracy 0.93 Test accuracy 0.85\n",
      "Train Epoch 208: Loss: 0.183157 Train accuracy 0.92 Test accuracy 0.86\n",
      "Train Epoch 209: Loss: 0.182414 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 210: Loss: 0.181232 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 211: Loss: 0.180845 Train accuracy 0.92 Test accuracy 0.86\n",
      "Train Epoch 212: Loss: 0.180719 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 213: Loss: 0.179872 Train accuracy 0.92 Test accuracy 0.86\n",
      "Train Epoch 214: Loss: 0.180789 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 215: Loss: 0.184054 Train accuracy 0.92 Test accuracy 0.85\n",
      "Train Epoch 216: Loss: 0.187759 Train accuracy 0.92 Test accuracy 0.86\n",
      "Train Epoch 217: Loss: 0.185576 Train accuracy 0.91 Test accuracy 0.86\n",
      "Train Epoch 218: Loss: 0.178899 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 219: Loss: 0.174996 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 220: Loss: 0.179083 Train accuracy 0.92 Test accuracy 0.86\n",
      "Train Epoch 221: Loss: 0.178692 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 222: Loss: 0.173793 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 223: Loss: 0.173662 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 224: Loss: 0.173946 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 225: Loss: 0.173630 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 226: Loss: 0.170649 Train accuracy 0.93 Test accuracy 0.85\n",
      "Train Epoch 227: Loss: 0.169771 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 228: Loss: 0.170983 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 229: Loss: 0.169416 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 230: Loss: 0.167756 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 231: Loss: 0.167329 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 232: Loss: 0.166822 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 233: Loss: 0.166932 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 234: Loss: 0.165034 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 235: Loss: 0.163775 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 236: Loss: 0.164162 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 237: Loss: 0.163647 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 238: Loss: 0.163026 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 239: Loss: 0.162119 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 240: Loss: 0.160510 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 241: Loss: 0.160310 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 242: Loss: 0.160090 Train accuracy 0.94 Test accuracy 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 243: Loss: 0.159342 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 244: Loss: 0.159062 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 245: Loss: 0.158250 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 246: Loss: 0.157457 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 247: Loss: 0.156791 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 248: Loss: 0.155762 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 249: Loss: 0.154824 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 250: Loss: 0.154255 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 251: Loss: 0.153631 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 252: Loss: 0.153078 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 253: Loss: 0.152897 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 254: Loss: 0.153146 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 255: Loss: 0.154010 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 256: Loss: 0.155189 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 257: Loss: 0.157165 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 258: Loss: 0.156349 Train accuracy 0.93 Test accuracy 0.86\n",
      "Train Epoch 259: Loss: 0.153302 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 260: Loss: 0.149180 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 261: Loss: 0.148238 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 262: Loss: 0.149984 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 263: Loss: 0.150910 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 264: Loss: 0.149233 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 265: Loss: 0.146335 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 266: Loss: 0.145307 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 267: Loss: 0.146138 Train accuracy 0.95 Test accuracy 0.87\n",
      "Train Epoch 268: Loss: 0.146551 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 269: Loss: 0.145724 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 270: Loss: 0.143678 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 271: Loss: 0.142354 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 272: Loss: 0.142499 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 273: Loss: 0.143060 Train accuracy 0.95 Test accuracy 0.85\n",
      "Train Epoch 274: Loss: 0.142558 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 275: Loss: 0.141293 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 276: Loss: 0.139856 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 277: Loss: 0.138864 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 278: Loss: 0.138817 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 279: Loss: 0.139024 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 280: Loss: 0.138694 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 281: Loss: 0.137861 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 282: Loss: 0.136930 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 283: Loss: 0.135886 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 284: Loss: 0.135131 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 285: Loss: 0.135028 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 286: Loss: 0.135072 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 287: Loss: 0.135008 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 288: Loss: 0.135029 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 289: Loss: 0.135175 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 290: Loss: 0.136341 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 291: Loss: 0.139347 Train accuracy 0.94 Test accuracy 0.85\n",
      "Train Epoch 292: Loss: 0.143055 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 293: Loss: 0.144560 Train accuracy 0.94 Test accuracy 0.86\n",
      "Train Epoch 294: Loss: 0.135369 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 295: Loss: 0.131311 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 296: Loss: 0.135551 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 297: Loss: 0.134232 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 298: Loss: 0.131995 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 299: Loss: 0.133056 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 300: Loss: 0.129771 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 301: Loss: 0.128306 Train accuracy 0.95 Test accuracy 0.85\n",
      "Train Epoch 302: Loss: 0.130856 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 303: Loss: 0.129612 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 304: Loss: 0.126436 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 305: Loss: 0.127403 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 306: Loss: 0.126243 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 307: Loss: 0.125202 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 308: Loss: 0.126187 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 309: Loss: 0.125283 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 310: Loss: 0.123600 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 311: Loss: 0.123557 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 312: Loss: 0.122920 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 313: Loss: 0.121819 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 314: Loss: 0.121747 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 315: Loss: 0.121637 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 316: Loss: 0.121238 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 317: Loss: 0.120706 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 318: Loss: 0.120616 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 319: Loss: 0.120444 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 320: Loss: 0.119890 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 321: Loss: 0.119415 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 322: Loss: 0.119597 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 323: Loss: 0.119189 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 324: Loss: 0.118686 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 325: Loss: 0.118499 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 326: Loss: 0.118632 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 327: Loss: 0.117975 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 328: Loss: 0.117265 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 329: Loss: 0.116343 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 330: Loss: 0.115466 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 331: Loss: 0.114268 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 332: Loss: 0.113384 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 333: Loss: 0.112944 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 334: Loss: 0.112620 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 335: Loss: 0.112054 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 336: Loss: 0.111622 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 337: Loss: 0.111747 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 338: Loss: 0.112470 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 339: Loss: 0.113913 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 340: Loss: 0.116865 Train accuracy 0.95 Test accuracy 0.85\n",
      "Train Epoch 341: Loss: 0.121891 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 342: Loss: 0.123881 Train accuracy 0.95 Test accuracy 0.85\n",
      "Train Epoch 343: Loss: 0.121053 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 344: Loss: 0.111728 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 345: Loss: 0.108518 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 346: Loss: 0.112983 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 347: Loss: 0.114711 Train accuracy 0.95 Test accuracy 0.86\n",
      "Train Epoch 348: Loss: 0.111347 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 349: Loss: 0.107462 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 350: Loss: 0.107930 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 351: Loss: 0.110231 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 352: Loss: 0.109769 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 353: Loss: 0.107086 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 354: Loss: 0.105017 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 355: Loss: 0.106013 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 356: Loss: 0.107586 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 357: Loss: 0.106369 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 358: Loss: 0.103993 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 359: Loss: 0.103068 Train accuracy 0.97 Test accuracy 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 360: Loss: 0.103973 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 361: Loss: 0.104196 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 362: Loss: 0.103273 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 363: Loss: 0.102260 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 364: Loss: 0.101531 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 365: Loss: 0.101116 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 366: Loss: 0.101354 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 367: Loss: 0.101541 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 368: Loss: 0.101092 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 369: Loss: 0.099972 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 370: Loss: 0.099090 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 371: Loss: 0.098760 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 372: Loss: 0.098749 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 373: Loss: 0.098730 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 374: Loss: 0.098505 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 375: Loss: 0.098261 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 376: Loss: 0.098108 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 377: Loss: 0.097871 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 378: Loss: 0.097248 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 379: Loss: 0.096539 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 380: Loss: 0.095903 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 381: Loss: 0.095420 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 382: Loss: 0.095106 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 383: Loss: 0.094815 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 384: Loss: 0.094342 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 385: Loss: 0.093921 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 386: Loss: 0.093618 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 387: Loss: 0.093521 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 388: Loss: 0.093649 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 389: Loss: 0.094167 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 390: Loss: 0.095477 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 391: Loss: 0.097280 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 392: Loss: 0.100748 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 393: Loss: 0.103110 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 394: Loss: 0.104383 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 395: Loss: 0.099580 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 396: Loss: 0.092332 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 397: Loss: 0.091748 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 398: Loss: 0.095859 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 399: Loss: 0.096651 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 400: Loss: 0.092999 Train accuracy 0.96 Test accuracy 0.86\n",
      "Train Epoch 401: Loss: 0.091282 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 402: Loss: 0.090722 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 403: Loss: 0.091175 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 404: Loss: 0.092827 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 405: Loss: 0.091910 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 406: Loss: 0.088529 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 407: Loss: 0.087648 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 408: Loss: 0.089362 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 409: Loss: 0.089795 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 410: Loss: 0.088294 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 411: Loss: 0.087278 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 412: Loss: 0.086887 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 413: Loss: 0.086111 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 414: Loss: 0.085811 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 415: Loss: 0.086407 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 416: Loss: 0.086518 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 417: Loss: 0.085436 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 418: Loss: 0.084631 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 419: Loss: 0.084508 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 420: Loss: 0.083956 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 421: Loss: 0.083343 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 422: Loss: 0.083120 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 423: Loss: 0.083413 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 424: Loss: 0.083313 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 425: Loss: 0.082990 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 426: Loss: 0.082635 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 427: Loss: 0.082560 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 428: Loss: 0.082166 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 429: Loss: 0.081571 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 430: Loss: 0.081069 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 431: Loss: 0.080838 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 432: Loss: 0.080681 Train accuracy 0.97 Test accuracy 0.87\n",
      "Train Epoch 433: Loss: 0.080268 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 434: Loss: 0.079853 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 435: Loss: 0.079553 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 436: Loss: 0.079457 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 437: Loss: 0.079551 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 438: Loss: 0.079509 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 439: Loss: 0.079739 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 440: Loss: 0.080214 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 441: Loss: 0.081230 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 442: Loss: 0.081580 Train accuracy 0.97 Test accuracy 0.85\n",
      "Train Epoch 443: Loss: 0.082225 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 444: Loss: 0.081078 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 445: Loss: 0.079887 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 446: Loss: 0.078216 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 447: Loss: 0.077063 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 448: Loss: 0.076190 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 449: Loss: 0.076299 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 450: Loss: 0.077357 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 451: Loss: 0.077853 Train accuracy 0.97 Test accuracy 0.86\n",
      "Train Epoch 452: Loss: 0.078111 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 453: Loss: 0.077728 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 454: Loss: 0.077223 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 455: Loss: 0.075744 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 456: Loss: 0.074624 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 457: Loss: 0.073840 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 458: Loss: 0.073503 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 459: Loss: 0.073950 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 460: Loss: 0.074458 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 461: Loss: 0.074724 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 462: Loss: 0.074602 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 463: Loss: 0.074754 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 464: Loss: 0.074393 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 465: Loss: 0.073859 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 466: Loss: 0.072826 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 467: Loss: 0.071741 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 468: Loss: 0.071053 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 469: Loss: 0.071060 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 470: Loss: 0.070966 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 471: Loss: 0.070608 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 472: Loss: 0.070713 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 473: Loss: 0.071191 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 474: Loss: 0.071831 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 475: Loss: 0.071900 Train accuracy 0.98 Test accuracy 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 476: Loss: 0.072538 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 477: Loss: 0.072493 Train accuracy 0.98 Test accuracy 0.85\n",
      "Train Epoch 478: Loss: 0.072937 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 479: Loss: 0.072220 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 480: Loss: 0.071118 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 481: Loss: 0.069203 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 482: Loss: 0.068038 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 483: Loss: 0.067943 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 484: Loss: 0.068022 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 485: Loss: 0.068072 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 486: Loss: 0.068322 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 487: Loss: 0.069080 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 488: Loss: 0.068848 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 489: Loss: 0.068282 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 490: Loss: 0.067177 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 491: Loss: 0.066760 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 492: Loss: 0.066204 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 493: Loss: 0.065544 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 494: Loss: 0.065004 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 495: Loss: 0.065043 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 496: Loss: 0.065297 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 497: Loss: 0.065252 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 498: Loss: 0.065476 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 499: Loss: 0.065647 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 500: Loss: 0.066304 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 501: Loss: 0.066346 Train accuracy 0.98 Test accuracy 0.85\n",
      "Train Epoch 502: Loss: 0.066459 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 503: Loss: 0.065590 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 504: Loss: 0.065101 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 505: Loss: 0.064143 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 506: Loss: 0.063154 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 507: Loss: 0.062255 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 508: Loss: 0.061915 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 509: Loss: 0.062061 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 510: Loss: 0.062296 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 511: Loss: 0.062582 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 512: Loss: 0.062720 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 513: Loss: 0.063565 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 514: Loss: 0.063897 Train accuracy 0.98 Test accuracy 0.85\n",
      "Train Epoch 515: Loss: 0.064447 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 516: Loss: 0.063714 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 517: Loss: 0.063347 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 518: Loss: 0.061950 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 519: Loss: 0.060741 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 520: Loss: 0.059608 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 521: Loss: 0.059250 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 522: Loss: 0.059876 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 523: Loss: 0.060536 Train accuracy 0.98 Test accuracy 0.85\n",
      "Train Epoch 524: Loss: 0.061116 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 525: Loss: 0.061111 Train accuracy 0.98 Test accuracy 0.85\n",
      "Train Epoch 526: Loss: 0.061480 Train accuracy 0.98 Test accuracy 0.87\n",
      "Train Epoch 527: Loss: 0.060788 Train accuracy 0.98 Test accuracy 0.86\n",
      "Train Epoch 528: Loss: 0.059857 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 529: Loss: 0.058260 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 530: Loss: 0.057350 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 531: Loss: 0.057357 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 532: Loss: 0.057802 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 533: Loss: 0.058231 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 534: Loss: 0.058293 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 535: Loss: 0.058545 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 536: Loss: 0.058362 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 537: Loss: 0.058103 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 538: Loss: 0.057125 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 539: Loss: 0.056157 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 540: Loss: 0.055440 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 541: Loss: 0.055262 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 542: Loss: 0.055311 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 543: Loss: 0.055308 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 544: Loss: 0.055559 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 545: Loss: 0.055881 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 546: Loss: 0.056507 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 547: Loss: 0.056473 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 548: Loss: 0.056719 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 549: Loss: 0.056263 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 550: Loss: 0.056072 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 551: Loss: 0.055306 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 552: Loss: 0.054541 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 553: Loss: 0.053270 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 554: Loss: 0.052546 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 555: Loss: 0.052711 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 556: Loss: 0.053027 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 557: Loss: 0.053212 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 558: Loss: 0.053372 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 559: Loss: 0.054067 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 560: Loss: 0.053987 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 561: Loss: 0.053832 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 562: Loss: 0.052888 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 563: Loss: 0.052217 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 564: Loss: 0.051485 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 565: Loss: 0.050914 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 566: Loss: 0.050319 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 567: Loss: 0.050226 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 568: Loss: 0.050593 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 569: Loss: 0.050808 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 570: Loss: 0.050975 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 571: Loss: 0.051092 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 572: Loss: 0.051621 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 573: Loss: 0.051626 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 574: Loss: 0.051589 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 575: Loss: 0.050645 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 576: Loss: 0.049895 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 577: Loss: 0.049068 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 578: Loss: 0.048585 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 579: Loss: 0.048058 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 580: Loss: 0.047823 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 581: Loss: 0.048071 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 582: Loss: 0.048387 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 583: Loss: 0.048850 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 584: Loss: 0.049008 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 585: Loss: 0.049464 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 586: Loss: 0.049347 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 587: Loss: 0.049439 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 588: Loss: 0.048449 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 589: Loss: 0.047488 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 590: Loss: 0.046440 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 591: Loss: 0.045990 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 592: Loss: 0.045883 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 593: Loss: 0.045923 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 594: Loss: 0.046154 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 595: Loss: 0.046388 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 596: Loss: 0.046794 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 597: Loss: 0.046717 Train accuracy 0.99 Test accuracy 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 598: Loss: 0.046602 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 599: Loss: 0.045982 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 600: Loss: 0.045587 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 601: Loss: 0.044893 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 602: Loss: 0.044181 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 603: Loss: 0.043722 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 604: Loss: 0.043720 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 605: Loss: 0.043947 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 606: Loss: 0.044053 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 607: Loss: 0.044174 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 608: Loss: 0.044246 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 609: Loss: 0.044700 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 610: Loss: 0.044762 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 611: Loss: 0.045135 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 612: Loss: 0.044823 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 613: Loss: 0.044707 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 614: Loss: 0.043978 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 615: Loss: 0.043299 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 616: Loss: 0.042226 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 617: Loss: 0.041502 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 618: Loss: 0.041486 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 619: Loss: 0.041854 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 620: Loss: 0.042145 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 621: Loss: 0.042144 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 622: Loss: 0.042168 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 623: Loss: 0.041877 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 624: Loss: 0.041575 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 625: Loss: 0.040845 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 626: Loss: 0.040235 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 627: Loss: 0.039903 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 628: Loss: 0.039911 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 629: Loss: 0.040003 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 630: Loss: 0.040101 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 631: Loss: 0.040403 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 632: Loss: 0.040610 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 633: Loss: 0.041022 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 634: Loss: 0.040959 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 635: Loss: 0.041083 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 636: Loss: 0.040631 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 637: Loss: 0.040222 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 638: Loss: 0.039227 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 639: Loss: 0.038402 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 640: Loss: 0.037951 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 641: Loss: 0.038009 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 642: Loss: 0.038316 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 643: Loss: 0.038579 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 644: Loss: 0.038966 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 645: Loss: 0.039071 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 646: Loss: 0.039335 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 647: Loss: 0.038823 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 648: Loss: 0.038351 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 649: Loss: 0.037533 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 650: Loss: 0.036884 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 651: Loss: 0.036432 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 652: Loss: 0.036336 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 653: Loss: 0.036523 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 654: Loss: 0.036770 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 655: Loss: 0.037000 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 656: Loss: 0.036876 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 657: Loss: 0.036725 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 658: Loss: 0.036284 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 659: Loss: 0.035892 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 660: Loss: 0.035375 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 661: Loss: 0.034992 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 662: Loss: 0.034845 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 663: Loss: 0.034875 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 664: Loss: 0.034969 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 665: Loss: 0.034993 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 666: Loss: 0.035129 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 667: Loss: 0.035144 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 668: Loss: 0.035290 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 669: Loss: 0.035116 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 670: Loss: 0.035012 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 671: Loss: 0.034625 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 672: Loss: 0.034294 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 673: Loss: 0.033760 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 674: Loss: 0.033336 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 675: Loss: 0.033084 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 676: Loss: 0.033013 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 677: Loss: 0.033036 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 678: Loss: 0.033089 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 679: Loss: 0.033275 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 680: Loss: 0.033442 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 681: Loss: 0.033809 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 682: Loss: 0.033977 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 683: Loss: 0.034515 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 684: Loss: 0.034608 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 685: Loss: 0.035255 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 686: Loss: 0.034829 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 687: Loss: 0.034403 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 688: Loss: 0.033011 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 689: Loss: 0.031865 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 690: Loss: 0.031352 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 691: Loss: 0.031609 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 692: Loss: 0.032305 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 693: Loss: 0.032709 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 694: Loss: 0.033154 Train accuracy 0.99 Test accuracy 0.87\n",
      "Train Epoch 695: Loss: 0.032673 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 696: Loss: 0.032053 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 697: Loss: 0.031061 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 698: Loss: 0.030458 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 699: Loss: 0.030343 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 700: Loss: 0.030697 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 701: Loss: 0.031254 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 702: Loss: 0.031358 Train accuracy 0.99 Test accuracy 0.86\n",
      "Train Epoch 703: Loss: 0.031365 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 704: Loss: 0.030763 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 705: Loss: 0.030147 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 706: Loss: 0.029537 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 707: Loss: 0.029316 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 708: Loss: 0.029448 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 709: Loss: 0.029664 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 710: Loss: 0.029883 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 711: Loss: 0.029761 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 712: Loss: 0.029532 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 713: Loss: 0.029088 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 714: Loss: 0.028704 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 715: Loss: 0.028434 Train accuracy 1.00 Test accuracy 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 716: Loss: 0.028336 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 717: Loss: 0.028375 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 718: Loss: 0.028437 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 719: Loss: 0.028519 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 720: Loss: 0.028471 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 721: Loss: 0.028383 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 722: Loss: 0.028154 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 723: Loss: 0.027911 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 724: Loss: 0.027633 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 725: Loss: 0.027432 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 726: Loss: 0.027275 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 727: Loss: 0.027182 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 728: Loss: 0.027144 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 729: Loss: 0.027166 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 730: Loss: 0.027246 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 731: Loss: 0.027268 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 732: Loss: 0.027366 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 733: Loss: 0.027316 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 734: Loss: 0.027360 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 735: Loss: 0.027154 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 736: Loss: 0.026940 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 737: Loss: 0.026576 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 738: Loss: 0.026294 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 739: Loss: 0.026047 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 740: Loss: 0.025927 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 741: Loss: 0.025875 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 742: Loss: 0.025906 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 743: Loss: 0.026048 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 744: Loss: 0.026159 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 745: Loss: 0.026385 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 746: Loss: 0.026344 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 747: Loss: 0.026366 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 748: Loss: 0.026053 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 749: Loss: 0.025738 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 750: Loss: 0.025280 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 751: Loss: 0.024962 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 752: Loss: 0.024807 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 753: Loss: 0.024824 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 754: Loss: 0.024971 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 755: Loss: 0.025129 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 756: Loss: 0.025360 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 757: Loss: 0.025351 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 758: Loss: 0.025470 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 759: Loss: 0.025122 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 760: Loss: 0.024740 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 761: Loss: 0.024246 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 762: Loss: 0.023952 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 763: Loss: 0.023880 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 764: Loss: 0.023984 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 765: Loss: 0.024202 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 766: Loss: 0.024345 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 767: Loss: 0.024544 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 768: Loss: 0.024455 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 769: Loss: 0.024362 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 770: Loss: 0.023896 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 771: Loss: 0.023458 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 772: Loss: 0.023111 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 773: Loss: 0.023037 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 774: Loss: 0.023174 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 775: Loss: 0.023355 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 776: Loss: 0.023556 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 777: Loss: 0.023560 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 778: Loss: 0.023589 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 779: Loss: 0.023272 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 780: Loss: 0.022922 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 781: Loss: 0.022514 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 782: Loss: 0.022276 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 783: Loss: 0.022235 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 784: Loss: 0.022313 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 785: Loss: 0.022449 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 786: Loss: 0.022507 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 787: Loss: 0.022561 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 788: Loss: 0.022413 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 789: Loss: 0.022268 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 790: Loss: 0.021948 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 791: Loss: 0.021656 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 792: Loss: 0.021481 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 793: Loss: 0.021467 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 794: Loss: 0.021556 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 795: Loss: 0.021685 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 796: Loss: 0.021876 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 797: Loss: 0.021865 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 798: Loss: 0.021844 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 799: Loss: 0.021553 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 800: Loss: 0.021249 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 801: Loss: 0.020925 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 802: Loss: 0.020749 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 803: Loss: 0.020753 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 804: Loss: 0.020863 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 805: Loss: 0.021042 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 806: Loss: 0.021081 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 807: Loss: 0.021173 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 808: Loss: 0.020968 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 809: Loss: 0.020712 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 810: Loss: 0.020366 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 811: Loss: 0.020132 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 812: Loss: 0.020029 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 813: Loss: 0.020023 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 814: Loss: 0.020067 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 815: Loss: 0.020118 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 816: Loss: 0.020185 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 817: Loss: 0.020121 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 818: Loss: 0.020099 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 819: Loss: 0.019908 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 820: Loss: 0.019720 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 821: Loss: 0.019510 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 822: Loss: 0.019364 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 823: Loss: 0.019266 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 824: Loss: 0.019205 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 825: Loss: 0.019188 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 826: Loss: 0.019201 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 827: Loss: 0.019251 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 828: Loss: 0.019270 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 829: Loss: 0.019369 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 830: Loss: 0.019323 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 831: Loss: 0.019311 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 832: Loss: 0.019103 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 833: Loss: 0.018921 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 834: Loss: 0.018676 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 835: Loss: 0.018499 Train accuracy 1.00 Test accuracy 0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 836: Loss: 0.018402 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 837: Loss: 0.018376 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 838: Loss: 0.018399 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 839: Loss: 0.018448 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 840: Loss: 0.018554 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 841: Loss: 0.018561 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 842: Loss: 0.018596 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 843: Loss: 0.018442 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 844: Loss: 0.018279 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 845: Loss: 0.018014 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 846: Loss: 0.017815 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 847: Loss: 0.017708 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 848: Loss: 0.017682 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 849: Loss: 0.017716 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 850: Loss: 0.017771 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 851: Loss: 0.017861 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 852: Loss: 0.017851 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 853: Loss: 0.017866 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 854: Loss: 0.017685 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 855: Loss: 0.017494 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 856: Loss: 0.017275 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 857: Loss: 0.017132 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 858: Loss: 0.017060 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 859: Loss: 0.017061 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 860: Loss: 0.017119 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 861: Loss: 0.017164 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 862: Loss: 0.017246 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 863: Loss: 0.017211 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 864: Loss: 0.017193 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 865: Loss: 0.017003 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 866: Loss: 0.016821 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 867: Loss: 0.016613 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 868: Loss: 0.016489 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 869: Loss: 0.016453 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 870: Loss: 0.016492 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 871: Loss: 0.016568 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 872: Loss: 0.016589 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 873: Loss: 0.016637 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 874: Loss: 0.016527 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 875: Loss: 0.016433 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 876: Loss: 0.016228 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 877: Loss: 0.016057 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 878: Loss: 0.015944 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 879: Loss: 0.015901 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 880: Loss: 0.015915 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 881: Loss: 0.015949 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 882: Loss: 0.016008 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 883: Loss: 0.015998 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 884: Loss: 0.015976 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 885: Loss: 0.015832 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 886: Loss: 0.015702 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 887: Loss: 0.015539 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 888: Loss: 0.015426 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 889: Loss: 0.015370 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 890: Loss: 0.015349 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 891: Loss: 0.015366 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 892: Loss: 0.015382 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 893: Loss: 0.015418 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 894: Loss: 0.015394 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 895: Loss: 0.015369 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 896: Loss: 0.015243 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 897: Loss: 0.015113 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 898: Loss: 0.014973 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 899: Loss: 0.014879 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 900: Loss: 0.014825 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 901: Loss: 0.014798 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 902: Loss: 0.014803 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 903: Loss: 0.014817 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 904: Loss: 0.014857 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 905: Loss: 0.014837 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 906: Loss: 0.014823 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 907: Loss: 0.014702 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 908: Loss: 0.014603 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 909: Loss: 0.014461 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 910: Loss: 0.014361 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 911: Loss: 0.014300 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 912: Loss: 0.014272 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 913: Loss: 0.014281 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 914: Loss: 0.014310 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 915: Loss: 0.014379 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 916: Loss: 0.014374 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 917: Loss: 0.014357 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 918: Loss: 0.014220 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 919: Loss: 0.014087 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 920: Loss: 0.013951 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 921: Loss: 0.013853 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 922: Loss: 0.013817 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 923: Loss: 0.013823 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 924: Loss: 0.013857 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 925: Loss: 0.013875 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 926: Loss: 0.013897 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 927: Loss: 0.013835 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 928: Loss: 0.013761 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 929: Loss: 0.013628 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 930: Loss: 0.013512 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 931: Loss: 0.013425 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 932: Loss: 0.013385 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 933: Loss: 0.013386 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 934: Loss: 0.013408 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 935: Loss: 0.013456 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 936: Loss: 0.013454 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 937: Loss: 0.013416 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 938: Loss: 0.013301 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 939: Loss: 0.013182 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 940: Loss: 0.013073 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 941: Loss: 0.013008 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 942: Loss: 0.012983 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 943: Loss: 0.012978 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 944: Loss: 0.012992 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 945: Loss: 0.013002 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 946: Loss: 0.013028 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 947: Loss: 0.012976 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 948: Loss: 0.012919 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 949: Loss: 0.012789 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 950: Loss: 0.012686 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 951: Loss: 0.012620 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 952: Loss: 0.012580 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 953: Loss: 0.012561 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 954: Loss: 0.012551 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 955: Loss: 0.012563 Train accuracy 1.00 Test accuracy 0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 956: Loss: 0.012552 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 957: Loss: 0.012557 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 958: Loss: 0.012504 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 959: Loss: 0.012454 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 960: Loss: 0.012359 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 961: Loss: 0.012276 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 962: Loss: 0.012208 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 963: Loss: 0.012158 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 964: Loss: 0.012129 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 965: Loss: 0.012113 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 966: Loss: 0.012125 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 967: Loss: 0.012149 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 968: Loss: 0.012191 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 969: Loss: 0.012156 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 970: Loss: 0.012134 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 971: Loss: 0.012000 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 972: Loss: 0.011885 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 973: Loss: 0.011805 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 974: Loss: 0.011780 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 975: Loss: 0.011796 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 976: Loss: 0.011808 Train accuracy 1.00 Test accuracy 0.86\n",
      "Train Epoch 977: Loss: 0.011821 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 978: Loss: 0.011764 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 979: Loss: 0.011711 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 980: Loss: 0.011616 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 981: Loss: 0.011530 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 982: Loss: 0.011485 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 983: Loss: 0.011466 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 984: Loss: 0.011456 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 985: Loss: 0.011456 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 986: Loss: 0.011470 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 987: Loss: 0.011457 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 988: Loss: 0.011430 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 989: Loss: 0.011339 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 990: Loss: 0.011256 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 991: Loss: 0.011187 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 992: Loss: 0.011145 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 993: Loss: 0.011133 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 994: Loss: 0.011135 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 995: Loss: 0.011150 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 996: Loss: 0.011131 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 997: Loss: 0.011104 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 998: Loss: 0.011037 Train accuracy 1.00 Test accuracy 0.87\n",
      "Train Epoch 999: Loss: 0.010972 Train accuracy 1.00 Test accuracy 0.87\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "mlp.to(device)\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.005)\n",
    "n_epochs = 1000\n",
    "\n",
    "train_accuracy_history = []\n",
    "test_accuracy_history = []\n",
    "\n",
    "\n",
    "\n",
    "x_train_scaled_tensor = torch.tensor(x_train_scaled, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "for epoch in range(n_epochs):\n",
    "    # - You need to specify dtype when converting data to torch.tensor\n",
    "    # - Call the outputs of the model \"outputs\" like in the line below\n",
    "    # outputs =  mlp.forward(...)\n",
    "    # YOUR CODE HERE\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp(x_train_scaled_tensor)\n",
    "    loss = F.cross_entropy(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    if skip_training:\n",
    "        break\n",
    "\n",
    "    if (epoch % 1) == 0:\n",
    "        # Store the progress of training\n",
    "        with torch.no_grad():\n",
    "            # outputs is the output of the model produced with forward function\n",
    "            logits = outputs.cpu().data.numpy()\n",
    "            pred_train = logits.argmax(axis=1)\n",
    "            train_accuracy = accuracy_score(pred_train, y_train)\n",
    "\n",
    "            # Compute test error\n",
    "            x = torch.tensor(x_test_scaled, device=device, dtype=torch.float)\n",
    "            outputs = mlp.forward(x)\n",
    "            logits = outputs.cpu().data.numpy()\n",
    "            pred_test = logits.argmax(axis=1)\n",
    "            test_accuracy = accuracy_score(pred_test, y_test)\n",
    "            train_accuracy_history.append(train_accuracy)\n",
    "            test_accuracy_history.append(test_accuracy)\n",
    "            print('Train Epoch {}: Loss: {:.6f} Train accuracy {:.2f} Test accuracy {:.2f}'.format(\n",
    "                epoch, loss.item(), train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70918774, -1.27826323,  0.70309086, ..., -0.55368838,\n",
       "         1.24844576,  0.59096275],\n",
       "       [ 0.13982666,  1.63495282, -2.12126847, ...,  1.62899005,\n",
       "         0.78236054, -0.58582052],\n",
       "       [-0.63200461, -0.36788322, -0.33021133, ..., -0.49132614,\n",
       "        -0.48272792, -1.51043595],\n",
       "       ..., \n",
       "       [-0.16890585, -0.67134322,  0.70309086, ...,  1.75371453,\n",
       "         0.38285892, -0.24959673],\n",
       "       [-0.0145396 ,  1.75633682, -2.19015528, ...,  0.9430054 ,\n",
       "         0.31627532, -1.25826811],\n",
       "       [-1.1722865 , -0.73203522, -0.67464539, ...,  0.13229627,\n",
       "        -0.34956071, -1.25826811]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy should be comparable to the accuracy of the random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43a179bfe2aaecbafa492f455d9e4694",
     "grade": true,
     "grade_id": "mlp_accuracy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save the model (type yes to confirm)? yes\n",
      "Model saved to 1_mlp.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the network to a file, submit this file together with your notebook\n",
    "filename = '1_mlp.pth'\n",
    "if not skip_training:\n",
    "    try:\n",
    "        do_save = input('Do you want to save the model (type yes to confirm)? ').lower()\n",
    "        if do_save == 'yes':\n",
    "            torch.save(mlp.state_dict(), filename)\n",
    "            print('Model saved to %s' % filename)\n",
    "        else:\n",
    "            print('Model not saved')\n",
    "    except:\n",
    "        raise Exception('The notebook should be run or validated with skip_training=False.')\n",
    "else:\n",
    "    mlp = MLP()\n",
    "    mlp.load_state_dict(torch.load(filename, map_location=lambda storage, loc: storage))\n",
    "    print('Model loaded from %s' % filename)\n",
    "    mlp = mlp.to(device)\n",
    "    mlp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1408a9336753e05a66921a09ada8acca",
     "grade": false,
     "grade_id": "cell-ffa1381201dcd251",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f33f1271710>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us plot the accuracies during training\n",
    "fig, ax = plt.subplots(1, figsize=(10, 5))\n",
    "ax.plot(train_accuracy_history, label='train')\n",
    "ax.plot(test_accuracy_history, label='test')\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6863798cde48e5895341793793d12196",
     "grade": false,
     "grade_id": "cell-339dff5131009354",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      1064\n",
      "           1       0.64      0.63      0.64       236\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1300\n",
      "   macro avg       0.78      0.78      0.78      1300\n",
      "weighted avg       0.87      0.87      0.87      1300\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(15,0.5,'true labels')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's print the classification report and plot the confusion matrix similarly to the random forest classifier.\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, pred_test))\n",
    "\n",
    "c_matrix = confusion_matrix(y_test, pred_test)\n",
    "fig, ax = plt.subplots(1, figsize=(4, 4))\n",
    "ax.set_title(\"Confusion matrix\")\n",
    "sns.heatmap(c_matrix, cmap='Blues', annot=True, fmt='g', cbar=False)\n",
    "ax.set_xlabel('predictions')\n",
    "ax.set_ylabel('true labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "750c239eaa2372aeea5e6154ee2b6f4b",
     "grade": false,
     "grade_id": "cell-ebc09a57c4ef9976",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Note that even though the random forest classifier may have better performance, the performance of the MLP network may be improved by tuning the hyperparameters, such as:\n",
    "* number of hidden units\n",
    "* number of layers\n",
    "* learning rate schedule\n",
    "* regularization methods.\n",
    "\n",
    "The message from this exercise is that in simple problems that do not have spatial (like in images) or temporal (like in time series) structure, alternative classifiers (like random forests) may do very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5478f97bfe97677a086b82e96d9ede7d",
     "grade": false,
     "grade_id": "cell-94367a22998608a6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Define an MLP with an arbitrary number of layers\n",
    "\n",
    "Let us now define a multilayer perceptron with an arbitrary number of layers and arbitrary number of neurons in each layer, so that an MLP can be created as follows:\n",
    "```python\n",
    "> mlp = FancyMLP([11, 150, 100, 50, 2], activation_fn=F.tanh)\n",
    "```\n",
    "In the example above, we created a network with three hidden layers: 150 units in the first hidden layer, 100 units in the second one and 50 units in the third one.\n",
    "\n",
    "Note: The same activation function should be applied to all the layers except for the last one. This way the MLP can be used either for regression or classification.\n",
    "\n",
    "Hints:\n",
    "* You may find it useful to use function [`torch.nn.Module.add_module`](https://pytorch.org/docs/master/nn.html#torch.nn.Module.add_module) or class [`torch.nn.ModuleList`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleList).\n",
    "* Check how many trainable parameters a created MLP has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e68c9905776a8285d6d7a49edf390de",
     "grade": false,
     "grade_id": "cell-02639f9ae9ca2c7c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class FancyMLP(nn.Module):\n",
    "    def __init__(self, sizes, activation_fn=torch.tanh):\n",
    "        \"\"\"Multilayer perceptron with an arbitrary number of layers.\n",
    "        \n",
    "        Args:\n",
    "          sizes (list):             Number of units in each layer including the input and the output layer:\n",
    "                                    [n_inputs, n_units_in_hidden_layer1, ..., n_units_in_hidden_layerN, n_outputs]\n",
    "          activation_fn (callable): An element-wise function used in every layer except in the last one.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        super(FancyMLP, self).__init__()\n",
    "        ##self.linears = nn.ModuleList([nn.Linear(sizes) for i in range(len(sizes)-1)])\n",
    "        ##self.out = nn.Linear(sizes[-2], sizes[-1])\n",
    "        self.n_layers = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 2):\n",
    "            self.n_layers.append(nn.Linear(sizes[i], sizes[i+1]))       \n",
    "        self.output_layer = nn.Linear(sizes[-2], sizes[-1])\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        #for i, l in enumerate(self.linears):\n",
    "        #    #x = self.linears[i // 2](x) + l(x)\n",
    "        #    x = activation_fn(self.linears(x))\n",
    "        #x = self.out(x)\n",
    "        for layer in self.n_layers:\n",
    "            x = layer(x)\n",
    "            x = torch.tanh(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "        \n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36bcec862f655931e3615b08ba016771",
     "grade": true,
     "grade_id": "fancy_mlp",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let us now test your class\n",
    "mlp = FancyMLP([n_inputs, 100, 50, 2])\n",
    "y = mlp(torch.randn(10, n_inputs))\n",
    "assert y.shape == torch.Size([10, 2]), \"Bad shape of y: y.shape={}\".format(y.shape)\n",
    "\n",
    "mlp = FancyMLP([3, 10, 30, 40, 50, 5])\n",
    "y = mlp(torch.randn(10, 3))\n",
    "assert y.shape == torch.Size([10, 5]), \"Bad shape of y: y.shape={}\".format(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2fdc53bcc62d2b9103d8a83d599664d",
     "grade": true,
     "grade_id": "fancy_mlp_nparams",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FancyMLP(\n",
       "  (n_layers): ModuleList(\n",
       "    (0): Linear(in_features=3, out_features=10, bias=True)\n",
       "    (1): Linear(in_features=10, out_features=30, bias=True)\n",
       "    (2): Linear(in_features=30, out_features=40, bias=True)\n",
       "    (3): Linear(in_features=40, out_features=50, bias=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=50, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the MLP\n",
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
