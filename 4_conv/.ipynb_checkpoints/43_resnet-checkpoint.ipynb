{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3cef3963a310e8bc9e86b71e08ebc0c",
     "grade": false,
     "grade_id": "cell-440df6cfa709812f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 4. Convolutional networks\n",
    "\n",
    "## Part 3. ResNet\n",
    "\n",
    "In the third part you need to train a convolutional neural network with a ResNet architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e2970339980ef7d85c3754662c4ee8",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b0b185c50941111bc63f11990e412ab",
     "grade": false,
     "grade_id": "cell-e7fb713b137e5e3a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# Select data directory\n",
    "import os\n",
    "if os.path.isdir('/coursedata'):\n",
    "    course_data_dir = '/coursedata'\n",
    "elif os.path.isdir('../data'):\n",
    "    course_data_dir = '../data'\n",
    "else:\n",
    "    # Specify course_data_dir on your machine\n",
    "    # course_data_dir = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e0d4635b5905e98963ce92fbfb03375",
     "grade": false,
     "grade_id": "cell-d1a0f2d597ac9692",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48d33ffe246f5459117f53cac15b370d",
     "grade": false,
     "grade_id": "cell-fe95dcf02c6b9c5e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f2b11aa8f0d0377563333bd78493751",
     "grade": false,
     "grade_id": "cell-e5b565cc4aae8e7f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## FashionMNIST dataset\n",
    "\n",
    "Let us use the FashionMNIST dataset. It consists of 60,000 training images of 10 classes: 'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05dae4d3d681cf9666b90ac940628a83",
     "grade": false,
     "grade_id": "cell-8b0fded08998282c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in /coursedata/fashion_mnist\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Transform to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Min-max scaling to [-1, 1]\n",
    "])\n",
    "\n",
    "data_dir = os.path.join(course_data_dir, 'fashion_mnist')\n",
    "print('Data stored in %s' % data_dir)\n",
    "trainset = torchvision.datasets.FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
    "           'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9515e202d9b9f214b97032c7fc387cd2",
     "grade": false,
     "grade_id": "cell-7236520c2892e5d1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This function computes the accuracy on the test dataset\n",
    "def compute_accuracy(net, testloader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49ddeb203278e46e4aaa122fac6664eb",
     "grade": false,
     "grade_id": "cell-e505c3b987b78603",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## ResNet\n",
    "\n",
    "Let us train a network with an architecure inspired by [ResNet](https://arxiv.org/pdf/1512.03385.pdf).\n",
    "\n",
    "### ResNet block\n",
    "Our ResNet consists of blocks with two convolutional layers and a skip connection.\n",
    "\n",
    "In the most general case, our implementation should have:\n",
    "\n",
    "<img src=\"resnet_block_04.png\" width=220 style=\"float: right;\">\n",
    "\n",
    "* Two convolutional layers with:\n",
    "    * 3x3 kernel\n",
    "    * no bias terms\n",
    "    * padding with one pixel on both sides\n",
    "    * 2d batch normalization after each convolutional layer.\n",
    "\n",
    "* **The first convolutional layer also (optionally) has:**\n",
    "    * different number of input channels and output channels\n",
    "    * change of the resolution with stride.\n",
    "\n",
    "* The skip connection:\n",
    "    * simply copies the input if the resolution and the number of channels do not change.\n",
    "    * If either the resolution or the number of channels change, the skip connection should have one convolutional layer with:\n",
    "        * 1x1 convolution **without bias**\n",
    "        * change of the resolution with stride (optional)\n",
    "        * different number of input channels and output channels (optional)\n",
    "    * If either the resolution or the number of channels change, the 1x1 convolutional layer is followed by 2d batch normalization.\n",
    "\n",
    "* The ReLU nonlinearity is applied after the first convolutional layer and at the end of the block.\n",
    "\n",
    "**Note: Batch normalization is expected to be right after a convolutional layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc44ddc69f370f1b5f1d68c0dfb0732f",
     "grade": false,
     "grade_id": "cell-0db2cc6de47fa70d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"resnet_blocks_123.png\" width=650 style=\"float: top;\">\n",
    "\n",
    "The implementation should also handle specific cases such as:\n",
    "\n",
    "Left: The number of channels and the resolution do not change.\n",
    "There are no computations in the skip connection.\n",
    "\n",
    "Middle: The number of channels changes, the resolution does not change.\n",
    "\n",
    "Right: The number of channels does not change, the resolution changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c970ca322994ba8941f555de4baf9b5",
     "grade": false,
     "grade_id": "cell-7c95703d5b7fa14c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Your task is to implement this block. You should use the implementations of layers in `nn.Conv2d`, `nn.BatchNorm2d` as the tests rely on those implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d38adfbfa37edfb193033cf63136c4e",
     "grade": false,
     "grade_id": "cell-5694742fd919140f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_channels (int):  Number of input channels.\n",
    "          out_channels (int): Number of output channels.\n",
    "          stride (int):       Controls the stride.\n",
    "        \"\"\"\n",
    "        super(Block, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(self.in_channels, self.out_channels, kernel_size=3, padding=1, stride=self.stride, bias=False)\n",
    "        self.conv_bn1 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_bn2 = nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "        self.conv_skip = nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1, stride=self.stride, bias=False)\n",
    "        self.conv_bn_skip = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        if (self.in_channels != self.out_channels) or (self.stride != 1):\n",
    "            skip = self.conv_bn_skip(self.conv_skip(x))\n",
    "            out = F.relu(self.conv_bn1(self.conv1(x)))\n",
    "            out = self.conv_bn2(self.conv2(out))\n",
    "            out += skip\n",
    "            out = F.relu(out)\n",
    "        else:\n",
    "            skip = x\n",
    "            out = F.relu(self.conv_bn1(self.conv1(x)))\n",
    "            out = self.conv_bn2(self.conv2(out))\n",
    "            out += skip\n",
    "            out = F.relu(out)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0e38a695cc58c087fb944435b432e76",
     "grade": true,
     "grade_id": "block",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shapes seem to be ok.\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation of the Block\n",
    "\n",
    "# The number of channels and resolution do not change\n",
    "batch_size = 20\n",
    "x = torch.zeros(batch_size, 16, 28, 28)\n",
    "block = Block(in_channels=16, out_channels=16)\n",
    "y = block(x)\n",
    "assert y.shape == torch.Size([batch_size, 16, 28, 28]), \"Bad shape of y: y.shape={}\".format(y.shape)\n",
    "\n",
    "# Increase the number of channels\n",
    "block = Block(in_channels=16, out_channels=32)\n",
    "y = block(x)\n",
    "assert y.shape == torch.Size([batch_size, 32, 28, 28]), \"Bad shape of y: y.shape={}\".format(y.shape)\n",
    "\n",
    "# Decrease the resolution\n",
    "block = Block(in_channels=16, out_channels=16, stride=2)\n",
    "y = block(x)\n",
    "assert y.shape == torch.Size([batch_size, 16, 14, 14]), \"Bad shape of y: y.shape={}\".format(y.shape)\n",
    "\n",
    "# Increase the number of channels and decrease the resolution\n",
    "block = Block(in_channels=16, out_channels=32, stride=2)\n",
    "y = block(x)\n",
    "assert y.shape == torch.Size([batch_size, 32, 14, 14]), \"Bad shape of y: y.shape={}\".format(y.shape)\n",
    "\n",
    "print('The shapes seem to be ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53ed1ca9da16198ce7eb9a79db3efce7",
     "grade": true,
     "grade_id": "block_relus",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51b00d6afb30e112db879c4c22150b81",
     "grade": true,
     "grade_id": "block_batch_norm",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell used for grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c1c1c641143426800b594ba413f0bb0",
     "grade": false,
     "grade_id": "cell-0162fb4406cb8e15",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Group of blocks\n",
    "\n",
    "ResNet consists of several groups of blocks. The first block in a group may change the number of channels (often multiples the number by 2) and subsample (using strides).\n",
    "\n",
    "<img src=\"resnet_group.png\" width=500 style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e619dcb4f94df39e95c1c84b81cd3165",
     "grade": false,
     "grade_id": "cell-b1930fef0ab076c3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let us implement a group of blocks in this cell\n",
    "class GroupOfBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_blocks, stride=1):\n",
    "        super(GroupOfBlocks, self).__init__()\n",
    "\n",
    "        first_block = Block(in_channels, out_channels, stride)\n",
    "        other_blocks = [Block(out_channels, out_channels) for _ in range(1, n_blocks)]\n",
    "        self.group = nn.Sequential(first_block, *other_blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.group(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ae5fb47e1118b4873e93d55ed8b1c9e",
     "grade": false,
     "grade_id": "cell-7481f327aee03c38",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupOfBlocks(\n",
      "  (group): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv_bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv_bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_skip): Conv2d(10, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (conv_bn_skip): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv_bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv_bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_skip): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (conv_bn_skip): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv_bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv_bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_skip): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (conv_bn_skip): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Let's test it\n",
    "group = GroupOfBlocks(in_channels=10, out_channels=20, n_blocks=3)\n",
    "print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "506af15e210ba69d4fed34ed84c83234",
     "grade": false,
     "grade_id": "cell-71e97cde35d51918",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### ResNet\n",
    "\n",
    "Let us mplement a ResNet with the following architecture. It contains three groups of blocks, each group having two basic blocks.\n",
    "\n",
    "<img src=\"resnet.png\" width=900 style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf6f75ad5ead6e051481f9ceba24e5bb",
     "grade": false,
     "grade_id": "cell-f0c465ad6f77bfc9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The cell below contains the implementation of all the layers except for the groups of blocks. Your task is to insert the groups of blocks in the middle.\n",
    "\n",
    "Note:\n",
    "* The number of channels in the second **group** should be double the number of channels in the first **group**, the number of channels in the third **group** should be four times the number of channels in the first **group**.\n",
    "* The second and the third **group** should reduce the resolution by `stride=2`, as shown in the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4887634d368b55faf02954f91902e2fd",
     "grade": false,
     "grade_id": "cell-b44918fa89e59c40",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, n_blocks, n_channels=64, num_classes=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          n_blocks (list):   A list with three elements which contains the number of blocks in \n",
    "                             each of the three groups of blocks in ResNet.\n",
    "                             For instance, n_blocks = [2, 4, 6] means that the first group has two blocks,\n",
    "                             the second group has four blocks and the third one has six blocks.\n",
    "          n_channels (int):  Number of channels in the first group of blocks.\n",
    "          num_classes (int): Number of classes.\n",
    "        \"\"\"\n",
    "        assert len(n_blocks) == 3, \"The number of groups should be three.\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_channels, kernel_size=5, stride=1, padding=2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(n_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        self.first_group = GroupOfBlocks(in_channels=n_channels, out_channels=n_channels, n_blocks=n_blocks[0])\n",
    "        self.second_group = GroupOfBlocks(in_channels=n_channels, out_channels=n_channels*2, n_blocks=n_blocks[1], stride=2)\n",
    "        self.third_group = GroupOfBlocks(in_channels=n_channels*2, out_channels=n_channels*4, n_blocks=n_blocks[2], stride=2)\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=4, stride=1)\n",
    "        self.fc = nn.Linear(4*n_channels, num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        if verbose: print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        if verbose: print('conv1:  ', x.shape)\n",
    "        x = self.bn1(x)\n",
    "        if verbose: print('bn1:    ', x.shape)\n",
    "        x = self.relu(x)\n",
    "        if verbose: print('relu:   ', x.shape)\n",
    "        x = self.maxpool(x)\n",
    "        if verbose: print('maxpool:', x.shape)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        x = self.first_group(x)\n",
    "        x = self.second_group(x)\n",
    "        x = self.third_group(x)\n",
    "        \n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        if verbose: print('avgpool:', x.shape)\n",
    "\n",
    "        x = x.view(-1, self.fc.in_features)\n",
    "        if verbose: print('x.view: ', x.shape)\n",
    "        x = self.fc(x)\n",
    "        if verbose: print('out:    ', x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bceef5e14b8945c01ec62757eb7f043",
     "grade": false,
     "grade_id": "cell-bae324fd2f70f08e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input tensor: torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32, 1, 28, 28])\n",
      "conv1:   torch.Size([32, 10, 28, 28])\n",
      "bn1:     torch.Size([32, 10, 28, 28])\n",
      "relu:    torch.Size([32, 10, 28, 28])\n",
      "maxpool: torch.Size([32, 10, 14, 14])\n",
      "avgpool: torch.Size([32, 40, 1, 1])\n",
      "x.view:  torch.Size([32, 40])\n",
      "out:     torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "The shapes to be ok.\n"
     ]
    }
   ],
   "source": [
    "# Create a network with 2 block in each of the three groups\n",
    "n_blocks = [2, 2, 2]  # number of blocks in the three groups\n",
    "net = ResNet(n_blocks, n_channels=10)\n",
    "net.to(device)\n",
    "\n",
    "# Feed a batch of images from the training data to test the network\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.to(device)\n",
    "    print('Shape of the input tensor:', images.shape)\n",
    "\n",
    "    y = net.forward(images, verbose=True)\n",
    "    print(y.shape)\n",
    "    assert y.shape == torch.Size([32, 10]), \"Bad shape of y: y.shape={}\".format(y.shape)\n",
    "\n",
    "print('The shapes to be ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d0761bf8b0e60b55728ba5981db176f",
     "grade": false,
     "grade_id": "cell-15b052b156ee3862",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (first_group): GroupOfBlocks(\n",
       "    (group): Sequential(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv_bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv_bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv_skip): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (conv_bn_skip): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv_bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv_bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv_skip): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (conv_bn_skip): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (second_group): GroupOfBlocks(\n",
       "    (group): Sequential(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(10, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (conv_bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv_bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv_skip): Conv2d(10, 20, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (conv_bn_skip): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv_bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv_bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv_skip): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (conv_bn_skip): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (third_group): GroupOfBlocks(\n",
       "    (group): Sequential(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (conv_bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv_bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv_skip): Conv2d(20, 40, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (conv_bn_skip): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv_bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv_bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv_skip): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (conv_bn_skip): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=4, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=40, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us print the architecture of the network\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71ad23804ee4f20b1aa0ff5499a14dae",
     "grade": false,
     "grade_id": "cell-9de9a4ddf89efc42",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let us now train the ResNet using the same training loop\n",
    "n_blocks = [2, 2, 2]  # number of blocks in the three groups\n",
    "net = ResNet(n_blocks, n_channels=16)\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58659a9b7f0f1499637f04664e6db398",
     "grade": false,
     "grade_id": "cell-50efe3e46b071ec9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.811\n",
      "[1,   400] loss: 0.551\n",
      "[1,   600] loss: 0.478\n",
      "[1,   800] loss: 0.464\n",
      "[1,  1000] loss: 0.401\n",
      "[1,  1200] loss: 0.394\n",
      "[1,  1400] loss: 0.355\n",
      "[1,  1600] loss: 0.338\n",
      "[1,  1800] loss: 0.339\n",
      "Accuracy of the network on the test images: 0.877\n",
      "[2,   200] loss: 0.450\n",
      "[2,   400] loss: 0.352\n",
      "[2,   600] loss: 0.344\n",
      "[2,   800] loss: 0.332\n",
      "[2,  1000] loss: 0.319\n",
      "[2,  1200] loss: 0.299\n",
      "[2,  1400] loss: 0.325\n",
      "[2,  1600] loss: 0.315\n",
      "[2,  1800] loss: 0.304\n",
      "Accuracy of the network on the test images: 0.891\n",
      "[3,   200] loss: 0.286\n",
      "[3,   400] loss: 0.303\n",
      "[3,   600] loss: 0.287\n",
      "[3,   800] loss: 0.264\n",
      "[3,  1000] loss: 0.301\n",
      "[3,  1200] loss: 0.266\n",
      "[3,  1400] loss: 0.266\n",
      "[3,  1600] loss: 0.277\n",
      "[3,  1800] loss: 0.279\n",
      "Accuracy of the network on the test images: 0.892\n",
      "[4,   200] loss: 0.239\n",
      "[4,   400] loss: 0.273\n",
      "[4,   600] loss: 0.253\n",
      "[4,   800] loss: 0.267\n",
      "[4,  1000] loss: 0.244\n",
      "[4,  1200] loss: 0.245\n",
      "[4,  1400] loss: 0.270\n",
      "[4,  1600] loss: 0.263\n",
      "[4,  1800] loss: 0.264\n",
      "Accuracy of the network on the test images: 0.897\n",
      "[5,   200] loss: 0.221\n",
      "[5,   400] loss: 0.232\n",
      "[5,   600] loss: 0.239\n",
      "[5,   800] loss: 0.231\n",
      "[5,  1000] loss: 0.244\n",
      "[5,  1200] loss: 0.252\n",
      "[5,  1400] loss: 0.231\n",
      "[5,  1600] loss: 0.248\n",
      "[5,  1800] loss: 0.233\n",
      "Accuracy of the network on the test images: 0.897\n",
      "[6,   200] loss: 0.216\n",
      "[6,   400] loss: 0.216\n",
      "[6,   600] loss: 0.224\n",
      "[6,   800] loss: 0.221\n",
      "[6,  1000] loss: 0.235\n",
      "[6,  1200] loss: 0.223\n",
      "[6,  1400] loss: 0.225\n",
      "[6,  1600] loss: 0.226\n",
      "[6,  1800] loss: 0.232\n",
      "Accuracy of the network on the test images: 0.901\n",
      "[7,   200] loss: 0.199\n",
      "[7,   400] loss: 0.212\n",
      "[7,   600] loss: 0.205\n",
      "[7,   800] loss: 0.211\n",
      "[7,  1000] loss: 0.221\n",
      "[7,  1200] loss: 0.218\n",
      "[7,  1400] loss: 0.214\n",
      "[7,  1600] loss: 0.213\n",
      "[7,  1800] loss: 0.217\n",
      "Accuracy of the network on the test images: 0.907\n",
      "[8,   200] loss: 0.184\n",
      "[8,   400] loss: 0.212\n",
      "[8,   600] loss: 0.188\n",
      "[8,   800] loss: 0.199\n",
      "[8,  1000] loss: 0.200\n",
      "[8,  1200] loss: 0.198\n",
      "[8,  1400] loss: 0.206\n",
      "[8,  1600] loss: 0.205\n",
      "[8,  1800] loss: 0.207\n",
      "Accuracy of the network on the test images: 0.895\n",
      "[9,   200] loss: 0.174\n",
      "[9,   400] loss: 0.184\n",
      "[9,   600] loss: 0.182\n",
      "[9,   800] loss: 0.189\n",
      "[9,  1000] loss: 0.187\n",
      "[9,  1200] loss: 0.188\n",
      "[9,  1400] loss: 0.193\n",
      "[9,  1600] loss: 0.197\n",
      "[9,  1800] loss: 0.188\n",
      "Accuracy of the network on the test images: 0.898\n",
      "[10,   200] loss: 0.175\n",
      "[10,   400] loss: 0.175\n",
      "[10,   600] loss: 0.180\n",
      "[10,   800] loss: 0.171\n",
      "[10,  1000] loss: 0.182\n",
      "[10,  1200] loss: 0.195\n",
      "[10,  1400] loss: 0.193\n",
      "[10,  1600] loss: 0.179\n",
      "[10,  1800] loss: 0.181\n",
      "Accuracy of the network on the test images: 0.906\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    print_every = 200  # mini-batches\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        # Transfer to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i % print_every) == (print_every-1):\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss/print_every))\n",
    "            running_loss = 0.0\n",
    "\n",
    "        if skip_training:\n",
    "            break\n",
    "    if skip_training:\n",
    "        break\n",
    "\n",
    "    # Print accuracy after every epoch\n",
    "    accuracy = compute_accuracy(net, testloader)\n",
    "    print('Accuracy of the network on the test images: %.3f' % accuracy)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08941fbd2953e25da378c4226c5a6488",
     "grade": false,
     "grade_id": "cell-6b2ab46dba3aa322",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "You should get the test accuracy at about 90-91%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69ea9b19ef5263efed4c58b3db96cf23",
     "grade": true,
     "grade_id": "cell-cd608a7294f2ceb6",
     "locked": true,
     "points": 0.001,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save the model (type yes to confirm)? yes\n",
      "Model saved to 4_resnet.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the network to a file, submit this file together with your notebook\n",
    "filename = '4_resnet.pth'\n",
    "if not skip_training:\n",
    "    try:\n",
    "        do_save = input('Do you want to save the model (type yes to confirm)? ').lower()\n",
    "        if do_save == 'yes':\n",
    "            torch.save(net.state_dict(), filename)\n",
    "            print('Model saved to %s' % filename)\n",
    "        else:\n",
    "            print('Model not saved')\n",
    "    except:\n",
    "        raise Exception('The notebook should be run or validated with skip_training=True.')\n",
    "else:\n",
    "    net = ResNet(n_blocks, n_channels=16)\n",
    "    net.load_state_dict(torch.load(filename, map_location=lambda storage, loc: storage))\n",
    "    net.to(device)\n",
    "    print('Model loaded from %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f420b734e91bcc49286c70c043995b25",
     "grade": true,
     "grade_id": "accuracy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 0.907\n"
     ]
    }
   ],
   "source": [
    "# Let us compute the accuracy on the test set\n",
    "accuracy = compute_accuracy(net, testloader)\n",
    "print('Accuracy of the network on the test images: %.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
